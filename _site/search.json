[
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "This is all the publications I have gotten round to uploading but it is missing a few. See ResearchGate and Google Scholar for a complete list.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nCOINr: An R package for developing composite indicators\n\n\nJournal of Open Source Software\n\n\n\nIndicators\n\n\nSoftware\n\n\n\n\n\n\nOct 11, 2022\n\n\nWilliam Becker, Giulio Caperna, Maria Del Sorbo, Hedvig Norlen, Eleni Papadimitriou, Michaela Saisana\n\n\n\n\n\n\n\nThe Water-Energy-Food Nexus Index: A Tool to Support Integrated Resource Planning, Management and Security\n\n\nFrontiers in Water\n\n\n\nIndicators\n\n\nWater\n\n\nPolicy\n\n\nSustainable development\n\n\n\n\n\n\nMar 10, 2022\n\n\nGareth Simpson, Graham Jewitt, William Becker, Jessica Badenhorst, Sara Masia, Ana Rita Neves, Pere Rovira, Victor Pascual\n\n\n\n\n\n\n\nExploring the link between Asia and Europe connectivity and sustainable development\n\n\nResearch in Globalization\n\n\n\nIndicators\n\n\nGlobalisation\n\n\nSustainable development\n\n\n\n\n\n\nDec 1, 2021\n\n\nWilliam Becker, Marcos Domínguez-Torreiro, Ana Neves, Carlos Tacao Moura, Michaela Saisana\n\n\n\n\n\n\n\nPolynomial chaos expansion for sensitivity analysis of model output with dependent inputs\n\n\nReliability Engineering & System Safety\n\n\n\nSensitivity analysis\n\n\nWater\n\n\nOptimisation\n\n\nMachine learning\n\n\n\n\n\n\nOct 1, 2021\n\n\nThierry Mara, William Becker\n\n\n\n\n\n\n\nA framework based on statistical analysis and stakeholders’ preferences to inform weighting in composite indicators\n\n\nEnvironmental Modelling and Software\n\n\n\nIndicators\n\n\nEnergy\n\n\nOptimisation\n\n\n\n\n\n\nSep 21, 2021\n\n\nDavid Linden, Marco Cinelli, Matteo Spada, admin , Patrick Gasser, Peter Burgherr\n\n\n\n\n\n\n\nA multidimensional high-resolution assessment approach to boost decentralised energy investments in Sub-Saharan Africa\n\n\nRenewable and Sustainable Energy Reviews\n\n\n\nIndicators\n\n\nEnergy\n\n\nPolicy\n\n\nSustainable development\n\n\n\n\n\n\nSep 1, 2021\n\n\nMagda Moner, Abbie Bender, William Becker, et al.\n\n\n\n\n\n\n\nA comprehensive comparison of total-order estimators for global sensitivity analysis\n\n\nInternational Journal for Uncertainty Quantification\n\n\n\nSensitivity analysis\n\n\nMonte Carlo\n\n\n\n\n\n\nJul 1, 2021\n\n\nArnald Puy, William Becker, Samuele Lo Piano, Andrea Saltelli\n\n\n\n\n\n\n\nVariable Selection in Regression Models Using Global Sensitivity Analysis\n\n\nJournal of Time Series Econometrics\n\n\n\nSensitivity analysis\n\n\nEconometrics\n\n\nModel selection\n\n\n\n\n\n\nMar 15, 2021\n\n\nWilliam Becker, Paulo Paruolo, Andrea Saltelli\n\n\n\n\n\n\n\nThe Future of Sensitivity Analysis: An essential discipline for systems modeling and policy support\n\n\nEnvironmental Modelling & Software\n\n\n\nSensitivity analysis\n\n\n\n\n\n\nMar 1, 2021\n\n\nSaman Razavi, Andrea Saltelli, Samuele Lo Piano, William Becker, Hoshin Gupta, Arnald Puy, Razi Sheikholeslami, et al.\n\n\n\n\n\n\n\nWrapping up the Europe 2020 strategy: A multidimensional indicator analysis\n\n\nEnvironmental and Sustainability Indicators\n\n\n\nIndicators\n\n\nPolicy\n\n\nSustainable development\n\n\n\n\n\n\nDec 1, 2020\n\n\nWilliam Becker, Hedvig Norlen, et al.\n\n\n\n\n\n\n\nMetafunctions for benchmarking in sensitivity analysis\n\n\nReliability Engineering & System Safety\n\n\n\nSensitivity analysis\n\n\nMachine learning\n\n\n\n\n\n\nDec 1, 2020\n\n\n\n\n\n\n\nDevelopment of a bioeconomy monitoring framework for the European Union: An integrative and collaborative approach\n\n\nNew Biotechnology\n\n\n\nIndicators\n\n\nPolicy\n\n\nSustainable development\n\n\n\n\n\n\nNov 25, 2020\n\n\nNicolas Robert, et al., William Becker\n\n\n\n\n\n\n\nThe Use of Quantitative Methods in the Policy Cycle\n\n\nScience for Policy Handbook\n\n\n\nSensitivity analysis\n\n\nPolicy\n\n\n\n\n\n\nJan 1, 2020\n\n\nGiuseppe Munda, Daniel Albrecht, William , Et al., Paolo Paruolo, Michaea Saisana\n\n\n\n\n\n\n\nWhy so many published sensitivity analyses are false: A systematic review of sensitivity analysis practices\n\n\nEnvironmental Modelling & Software\n\n\n\nSensitivity analysis\n\n\n\n\n\n\nApr 1, 2019\n\n\nAndrea Saltelli, William Becker, et al.\n\n\n\n\n\n\n\nGlobal sensitivity analysis for high-dimensional problems: How to objectively group factors and measure robustness and convergence while reducing computational cost\n\n\nEnvironmental Modelling & Software\n\n\n\nSensitivity analysis\n\n\nWater\n\n\n\n\n\n\nJan 1, 2019\n\n\nRazi Sheikholeslami, Saman Razavi, Hoshin Gupta, William Becker, Amin Haghnegahdar\n\n\n\n\n\n\n\nThe Water Retention Index: Using land use planning to manage water resources in Europe\n\n\nSustainable Development\n\n\n\nIndicators\n\n\nPolicy\n\n\nSustainable development\n\n\nWater\n\n\n\n\n\n\nApr 25, 2018\n\n\nIne Vandecasteele, William Becker, et al.\n\n\n\n\n\n\n\nSensitivity analysis approaches to high-dimensional screening problems at low sample size\n\n\nJournal of Statistical Computation and Simulation\n\n\n\nSensitivity analysis\n\n\nMonte Carlo\n\n\n\n\n\n\nApr 4, 2018\n\n\nWilliam Becker, Stefano Tarantola, Gregory Deman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/2022_COINr/index.html",
    "href": "publications/2022_COINr/index.html",
    "title": "COINr: An R package for developing composite indicators",
    "section": "",
    "text": "About\nThe COINr package, introduced in this article, is an R package that aims to provide a harmonised development environment for composite indicators that includes all common operations from indicator selection, data treatment and imputation up to aggregation, presentation of results and sensitivity analysis. COINr enables development, visualisation and exploration of methodological variations, and encourages transparency and reproducibility.\n\nCOINr is hosted at its GitHub page here. All documentation is here."
  },
  {
    "objectID": "publications/2021_solarafrica/index.html",
    "href": "publications/2021_solarafrica/index.html",
    "title": "A multidimensional high-resolution assessment approach to boost decentralised energy investments in Sub-Saharan Africa",
    "section": "",
    "text": "About\nThere are over 650 million people in Africa who have no access to electricity; this is in sharp contrast to the continent’s vast untapped renewable energy potential and due largely to the historical lack of investments in energy infrastructure. New investments in decentralised power generation within Sub-Saharan Africa play a progressively important role in increasing energy access and addressing the continent’s electricity supply shortages. Tracking the performance of Sub-Saharan African countries along various socio-political and economic axes can spur the mobilisation of private, public and international sectors in investing in decentralised energy technologies. An increasing amount of high-resolution global spatial data are available, and used for various assessments. However, key multidimensional indicators are mainly still provided only at the national level. To this end, we present a comprehensive and consistent analysis of the attractiveness for decentralised photovoltaic technologies at an unprecedented level of detail using both high-resolution spatial data and national reports. We develop and build a new composite indicator that considers the interplay between social, political, environmental and financial factors at a granular regional level for Sub-Saharan Africa and embeds within it the importance of the local production costs at high-spatial resolution.\n\nSee interactive data exploration here. See also accompanying Data in Brief article."
  },
  {
    "objectID": "publications/2021_PCE/index.html",
    "href": "publications/2021_PCE/index.html",
    "title": "Polynomial chaos expansion for sensitivity analysis of model output with dependent inputs",
    "section": "",
    "text": "About\nIn this paper, we discuss the sensitivity analysis of model response when the uncertain model inputs are not independent of one other. In this case, two different kinds of sensitivity indices can be evaluated: (i) the sensitivity indices that account for the dependence/correlation of an input or group of inputs with the remainder and (ii) the sensitivity indices that do not account for this dependence. We argue that this distinction applies to any global sensitivity measure. In the present work, we focus on the estimation of variance-based sensitivity indices which are based on the second-order moment of the model response of interest. In particular, we derive new strategies and new computationally efficient methods to assess them, which rely on the polynomial chaos expansion. Several numerical exercises are carried out to demonstrate the performance of the new methods, including a sensitivity analysis of a drainage model posterior to its statistical calibration."
  },
  {
    "objectID": "publications/2021_FutureSA/index.html",
    "href": "publications/2021_FutureSA/index.html",
    "title": "The Future of Sensitivity Analysis: An essential discipline for systems modeling and policy support",
    "section": "",
    "text": "About\nSensitivity analysis (SA) is en route to becoming an integral part of mathematical modeling. The tremendous potential benefits of SA are, however, yet to be fully realized, both for advancing mechanistic and data-driven modeling of human and natural systems, and in support of decision making. In this perspective paper, a multidisciplinary group of researchers and practitioners revisit the current status of SA, and outline research challenges in regard to both theoretical frameworks and their applications to solve real-world problems. Six areas are discussed that warrant further attention, including (1) structuring and standardizing SA as a discipline, (2) realizing the untapped potential of SA for systems modeling, (3) addressing the computational burden of SA, (4) progressing SA in the context of machine learning, (5) clarifying the relationship and role of SA to uncertainty quantification, and (6) evolving the use of SA in support of decision making. An outlook for the future of SA is provided that underlines how SA must underpin a wide variety of activities to better serve science and society."
  },
  {
    "objectID": "publications/2020_QuantPolicy/index.html",
    "href": "publications/2020_QuantPolicy/index.html",
    "title": "The Use of Quantitative Methods in the Policy Cycle",
    "section": "",
    "text": "About\nThis Chapter presents a set of quantitative modelling approaches, connected to various steps of the policy cycle, that aim at helping policy-makers and all social actors involved, by providing a scientific sound framework for a systematic, coherent and transparent analysis. Practical guidelines for structuring policy problems by using uncertainty and sensitivity analysis, multi-criteria decision analysis, composite indicators and ex-post impact evaluation are provided.\n\nSee the entire Science for Policy Handbook here."
  },
  {
    "objectID": "publications/2020_EU2020/index.html",
    "href": "publications/2020_EU2020/index.html",
    "title": "Wrapping up the Europe 2020 strategy: A multidimensional indicator analysis",
    "section": "",
    "text": "About\nThe Europe 2020 Strategy was launched by the European Commission in 2010 to promote smart, sustainable, and inclusive growth across EU member states. As the strategy draws to a close in 2020 and is superseded by the Sustainable Development Goals and the Green Deal, this work aims to assess the progress made over the last decade, and to carry forward lessons for future endeavours. A composite indicator approach is adopted, which aggregates the distance of each country or region to politically-agreed targets. This allows a high-level summary of progress, but also examines detailed trends at national and regional levels, as well as by degree of urbanisation and by development. The results show that although the EU has moved forward as whole, some regions have lagged behind or even moved backwards, and within some countries, regions are moving further away from one another. Progress has been particularly strong in education, but more work is needed in the environmental dimensions."
  },
  {
    "objectID": "publications/2019_WhySAFalse/index.html",
    "href": "publications/2019_WhySAFalse/index.html",
    "title": "Why so many published sensitivity analyses are false: A systematic review of sensitivity analysis practices",
    "section": "",
    "text": "About\nSensitivity analysis provides information on the relative importance of model input parameters and assumptions. It is distinct from uncertainty analysis, which addresses the question ‘How uncertain is the prediction?’ Uncertainty analysis needs to map what a model does when selected input assumptions and parameters are left free to vary over their range of existence, and this is equally true of a sensitivity analysis. Despite this, many uncertainty and sensitivity analyses still explore the input space moving along one-dimensional corridors leaving space of the input factors mostly unexplored. Our extensive systematic literature review shows that many highly cited papers (42% in the present analysis) fail the elementary requirement to properly explore the space of the input factors. The results, while discipline-dependent, point to a worrying lack of standards and recognized good practices. We end by exploring possible reasons for this problem, and suggest some guidelines for proper use of the methods."
  },
  {
    "objectID": "publications/2018_WaterRetention/index.html",
    "href": "publications/2018_WaterRetention/index.html",
    "title": "The Water Retention Index: Using land use planning to manage water resources in Europe",
    "section": "",
    "text": "About\nAppropriate land management can be an effective approach to improving water quantity regulation. There is, however, a need to identify both where measures are most needed and where they may be most effective. The water retention index (WRI) was developed with this goal in mind. The WRI is a composite indicator which takes into account parameters reflecting potential water retention in vegetation, water bodies, soil and underlying aquifers, as well as the influence of slope and artificially sealed areas. Three land management scenarios were simulated up to 2030 using the LUISA modeling platform: increasing grassland in upstream areas as well as afforestation in both upstream areas and riparian zones. The WRI was computed for all scenarios as well as a comparative “business-as-usual” baseline scenario. All scenarios showed an overall improvement of the index as compared to this baseline, with afforestation in upstream areas having the greatest effect. The WRI can provide useful insights into the current capacity of a landscape to regulate water as well as the effectiveness of possible remediation strategies applied at the European scale."
  },
  {
    "objectID": "projects/WEF/index.html",
    "href": "projects/WEF/index.html",
    "title": "Water-Energy-Food Nexus Index",
    "section": "",
    "text": "The WEF (Water-Energy-Food) framework consists of a series of indicators used since 2011 to assess availability and access to resources, and their sustainable development at a global level.\nI was contracted to do the main model calculation of the WEF Nexus Index, including global sensitivity analysis and providing expert advice."
  },
  {
    "objectID": "projects/UNIDO/index.html",
    "href": "projects/UNIDO/index.html",
    "title": "Quality Infrastructure for Sustainable Development Index",
    "section": "",
    "text": "Quality infrastructure (QI) includes standards, conformity assessment, accreditation and metrology, and is the foundation for international trade. QI promotes sustainable development, through promoting economic development and raising health and environmental standards, among other things.\nOver 2020-2022 I led the development of the Quality Infrastructure for Sustainable Development Index for the United Nations Industrial Development Organisation, which aims to measure at the national level, how each country’s QI is suitable for reaching sustainable development goals. This process involved:\n\nClient engagement\nLiterature review\nStakeholder engagement, expert workshops\nData collection (including web scraping), cleaning\nData analysis and processing, modelling\nPresentation, visualisation\nReporting and extracting policy messages\nObtaining feedback and making adjustments\nWorking with other consultants and professionals to deliver the best outcome"
  },
  {
    "objectID": "projects/Programme monitoring/index.html",
    "href": "projects/Programme monitoring/index.html",
    "title": "European programme monitoring",
    "section": "",
    "text": "I worked with the European Commission’s Directorate-General for Budget in analysing performance data for all European budget programmes, covering many billions of EUR, with the aim of providing a harmonised reporting framework.\nI also helped to develop guidelines for programme monitoring, for the present multi-annual financial framework, and advised in meetings with the Regulatory Scrutiny Board.\nThis was multi-year support work which involved data analysis and client engagement, as well as regularly attending meetings in Brussels and presenting work."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "European Innovation Scoreboard\n\n\nUpdating the European Innovation Scoreboard for the European Commission\n\n\n\nInnovation\n\n\nPolicy\n\n\nIndicators\n\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Skills Index\n\n\nUpdating and improving the European Skills Index for Cedefop.\n\n\n\nSkills\n\n\nPolicy\n\n\nIndicators\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDGAR: Global emissions database\n\n\nEDGAR is a highly detailed global database of greenhouse gas and pollution emissions maintained by the European Commission.\n\n\n\nSustainabilty\n\n\nClimate\n\n\nPolicy\n\n\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA2SIT app for community-based protection\n\n\nMeasuring migrant and refugee vulnerability in Guatemala.\n\n\n\nIndicators\n\n\nSustainable development\n\n\nPolicy\n\n\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining\n\n\nTraining courses, lecturing and tuition.\n\n\n\nOther\n\n\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVietnam provincial innovation index\n\n\nAuditing and assisting with an index to measure innovation at the regional level in Vietnam.\n\n\n\nInnovation\n\n\nPolicy\n\n\nIndicators\n\n\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComposer App\n\n\nA web app for building and analysing composite indicators\n\n\n\nSoftware\n\n\nIndicators\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOINr: An R package for composite indicators\n\n\nMy R package is used worldwide by professional developers.\n\n\n\nSoftware\n\n\nIndicators\n\n\n\n\nSep 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEU Bioeconomy Indicators\n\n\nTime series analysis for bioeconomy indicators.\n\n\n\nIndicators\n\n\nSustainable development\n\n\n\n\nAug 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Innovation Index\n\n\nCustom R package development for innovation modelling\n\n\n\nIndicators\n\n\nInnovation\n\n\nSoftware\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWater-Energy-Food Nexus Index\n\n\nModel calculation and global sensitivity analysis.\n\n\n\nIndicators\n\n\nSustainable development\n\n\n\n\nMar 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuality Infrastructure for Sustainable Development Index\n\n\nLinking quality infrastructure and sustainable development with a data-driven analysis.\n\n\n\nIndicators\n\n\nSustainable development\n\n\nPolicy\n\n\n\n\nDec 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther projects\n\n\nAn overview of other projects.\n\n\n\nOther\n\n\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOIN Tool\n\n\nAn advanced Excel tool for composite indicators\n\n\n\nSoftware\n\n\nIndicators\n\n\n\n\nNov 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean programme monitoring\n\n\nMonitoring European spending programmes.\n\n\n\nPolicy\n\n\n\n\nJun 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpact assessment\n\n\nEx-ante impact assessments for European policy making.\n\n\n\nPolicy\n\n\n\n\nJan 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsia-Europe Connectivity\n\n\nMeasuring sustainable connectivity between Europe and Asia\n\n\n\nIndicators\n\n\nSustainable development\n\n\nPolicy\n\n\n\n\nOct 20, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/GII/index.html",
    "href": "projects/GII/index.html",
    "title": "Global Innovation Index",
    "section": "",
    "text": "Over 2021-2022 I have developed a custom R package for the UN’s World Intellectual Property Organisation. This involved building a streamlined package capable of building the Global Innovation Index (a hierarchical composite indicator), including data screening, data treatment, normalisation and aggregation.\nThe package also allows ex-post checks on methodological assumptions, and automatically yields data for generating country profiles. This greatly saves time and dramatically reduces the possibility of errors in the complex model evaluation.\nThe package is hosted at a private repo on Github."
  },
  {
    "objectID": "projects/EIS/index.html",
    "href": "projects/EIS/index.html",
    "title": "European Innovation Scoreboard",
    "section": "",
    "text": "The European Innovation Scoreboard (EIS) is a system of indicators and indexes that is produced by the European Commission to measure innovation performance in EU countries, other European countries, and regional neighbours.\nIn 2023 I supported a consortium of companies, led by the EFIS Centre in bidding on a contract to update and develop the EIS over the next four years. My role in the bid was explaining how the index calculations would be automated in R, using my now-ubiquitous COINr package. Something must have gone right with our bid, because we won it and began work on this project in early 2024.\nMy work on this project has been to replicate the EIS, which was previously calculated over a series of spreadsheets, in R using COINr. More than that, we (with colleagues in Technopolis) set up a full data pipeline, leveraging APIs where possible, to make the process as fully automated and reproducible as possible.\nThis work was similar to the updates I performed on the European Skills Index, but had many more complexities: the index had a number of hidden quirks which had to be understood and replicated, and the data covered EU-neighbouring countries which made data collection challenging.\nIn the end, the result is a much-improved index page which has been built by friends in OneTandem. I think our work has greatly improved the presentation, the transparency and reproducibility of the index. Although this time round we simply replicated the methodology of previous years, the work has laid all the foundations to work on methodological improvements next time around!"
  },
  {
    "objectID": "projects/COINtool/index.html",
    "href": "projects/COINtool/index.html",
    "title": "COIN Tool",
    "section": "",
    "text": "The COIN Tool is an advanced Excel tool for building and auditing composite indicators. While working at the European Commission’s Joint Research Centre, I collaborated with a professional developer to bring the tool from a prototype to release. Aside from contributing to the development of the tool, I am the main author of the documentation.\nThe COIN Tool is now used worldwide as an accessible development tool by academics and researchers in policy making. Its documentation, can be found here."
  },
  {
    "objectID": "projects/COINr/index.html",
    "href": "projects/COINr/index.html",
    "title": "COINr: An R package for composite indicators",
    "section": "",
    "text": "COINr is a high-level R package which is the first fully-flexible development and analysis environment for composite indicators and scoreboards. The main features can be summarised as features for building, features for analysis and features for visualisation and presentation.\nI am the author, designer and developer of this package. It is used worldwide by universities and international organisations to develop and analyse composite indicators, including the European Commission’s Joint Research Centre, United Nations agencies and many others.\nCOINr was originally developed under contract with the European Commission’s Joint Research Centre, but has since been developed much futher under other contracts that I have worked on, and in my own time.\nFor further reading, see:\n\nThe main COINr website and documentation\nGithub repo\nCRAN page\nOnline book (this is now outdated and refers to an older syntax)\nPaper published in the Journal of Open Source Software"
  },
  {
    "objectID": "projects/ASEM/index.html",
    "href": "projects/ASEM/index.html",
    "title": "Asia-Europe Connectivity",
    "section": "",
    "text": "I co-led a major multi-year project to support the European Commission’s External Action Service in measuring international links between Asian and European countries. The work supported the high-level Asia-Europe Meeting in which heads of state of Asian and European countries meet to explore how to better connect on issues such as trade, education, research, defense and global issues such as climate change and health.\nThe work involved:\n\nExtensive client consultation\nOrganising two international workshops with topical experts from Asia and Europe\nCollecting and cleaning extensive amounts of data\nStatistical and multivariate analysis\nMeeting client needs while respecting technical rigour and practical considerations\nWriting and publication of a flagship report\nDesigning an interactive data exploration platform\nPresenting work to senior stakeholders (up to ministerial level)\nNavigating a sensitive political environment and respecting cultural differences\n\nTo find out more about this work, see the ASEM Sustainable Connectivity Portal.\nThe work also resulted in a spin-off international research conference (AESCON), which I co-founded and co-organised.\nSee also our academic paper on this topic."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n12 Lessons learned from four years of freelancing\n\n\nNuggets of wisdom? \n\n\n\n\n\nJuly 10, 2024\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nLaunch of the {composer} composite indicator app\n\n\nFinally the launch of this open-source web app for interactively building composite indicators! \n\n\n\n\n\nMay 10, 2024\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate and release of the European Skills Index\n\n\nA few technical details. \n\n\n\n\n\nMarch 1, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Wrap Up\n\n\nLooking back on a very busy and interesting year. \n\n\n\n\n\nDecember 31, 2023\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nUpdates on the FIND composite indicators app\n\n\nWrapping up the composite indicators app project for release. \n\n\n\n\n\nDecember 30, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nUpdates of the WEF Nexus Index\n\n\nUpdating the WEF Nexus Index with the latest data. \n\n\n\n\n\nNovember 20, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Innovation Scoreboard\n\n\nIn which we put in a bid to update and maintain this index. \n\n\n\n\n\nNovember 17, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nExpert group meeting on quality infrastructure in Vienna\n\n\nNovember 14-15, Vienna, Austria. Expert group meeting on the QI4SD Index in Vienna with representatives from INetQI organisations and national institutions. \n\n\n\n\n\nNovember 15, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n2023 Wrap Up\n\n\nOctober 3-4, Riyadh, Saudi Arabia. A workshop to discuss the QI4SD Index. \n\n\n\n\n\nOctober 5, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nCOIN training on composite indicators\n\n\nMy lectures on robustness in composite indicators and the COINr package. \n\n\n\n\n\nSeptember 28, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal emissions data set published\n\n\nWe published the JRC’s annual report on greenhouse gas emissions of all countries in the world. \n\n\n\n\n\nSeptember 8, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nA2SIT App launched for UNHCR\n\n\nThe Admin-2 Severity Index Tool was completed in its beta state and launched for UNHCR Guatemala. \n\n\n\n\n\nJuly 28, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nFreelance data science: a two-year review\n\n\nThe story of a two-year freelance journey and its pros and cons. \n\n\n\n\n\nDecember 16, 2022\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nNew projects plus website\n\n\nRecently I built a website, and I’m starting several new projects soon. Things starting to get very busy! \n\n\n\n\n\nDecember 5, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nI don’t like MS Teams\n\n\nWhy I really really don’t like Microsoft Teams in one short rant :smile: \n\n\n\n\n\nDecember 1, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nPaper for COINr published\n\n\nA little bit about my new paper and why it is a Good Thing. \n\n\n\n\n\nOctober 11, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nOpen peer review: a better way?\n\n\nUsing Github and transparent open review to improve the peer review process. \n\n\n\n\n\nSeptember 14, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenesis\n\n\nNew personal website launched! :rocket: \n\n\n\n\n\nSeptember 8, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nOccam’s Razor\n\n\nAnd its many applications. \n\n\n\n\n\nMarch 1, 2022\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html",
    "href": "posts/2024-06_Lessons/index.html",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "",
    "text": "As of May I started an exciting new chapter in my career-voyage, working as a training specialist in machine learning at the ECMWF in Bonn. This is a really nice move to get back towards ML and my love of training, and so far it is has been super interesting to learn about all the work based on deep neural networks going on there.\nOn the other hand, I’ve had to wrap up my freelance work, at least for the moment. Freelancing has been a hugely positive experience for me, and has taught me a lot of skills that would have been difficult to learn from within a big organisation. So, partly to celebrate a new step in my career, partly to share some possible nuggets of wisdom that I’ve gained over the last few years, and partly as some notes to myself in case I come back to freelancing at some point, here are my 12 Lessons Learned. This is probably the most click-baity title I have posted under so far, but I’m afraid we’ll all have to live with that :)\nSo without further ado, here we go."
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html#get-the-basics-right",
    "href": "posts/2024-06_Lessons/index.html#get-the-basics-right",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "Get the basics right 🔤",
    "text": "Get the basics right 🔤\nIf you want to do well as a freelancer, get the basics right.\nPut yourselves in the shoes of a customer (often, in my case, international organisations and consultancies): what do they want when they outsource work? I think, in general what they want is:\n\nThe work to get done, to a high standard\nDeadlines respected\nAn easy and respectful working relationship (NO DRAMAS!)\nGood communication\n\nOf course there are other requirements that are specific to the task, and different people will have different priorities. But I would say that those are the main things to pay attention to. It’s a pretty simple formula really, but sometimes these things can go astray. For example, if you don’t manage your time and workload effectively, you might easily miss deadlines - this creates extra work and uncertainty for your customers, and if it happens too often, can damage your reputation (which is key, see below). Equally, just answering emails within a good time frame is important, otherwise customers will waste time chasing you up. All of these simple things contribute to a smooth experience which is good for everyone."
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html#networking-is-key-but-no-need-to-shout",
    "href": "posts/2024-06_Lessons/index.html#networking-is-key-but-no-need-to-shout",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "Networking is key, but no need to shout 🤫",
    "text": "Networking is key, but no need to shout 🤫\nYour network, and your professional reputation, is essential in bringing in new contracts and new clients. Most of my work came through positive referrals from other clients, and this meant I was lucky enough to never have to go out and look for work - work always came to me.\nSurprisingly though, I don’t really do any “active” networking. I don’t cold-email potential clients, and I don’t sidle up to senior figures at conferences with the hope of scoring a new contract. I’m not saying there’s anything wrong with doing that at all, but it’s just not really my style (insert your preferred joke here about having no style). On the other hand, I am happy and genuinely interested to chat to other people about most things. And I am very open minded about taking on new work, and I love starting new projects with new people.\n\n\n\nNo need to get too intense\n\n\nAbove all though, I think that if you Get The Basics Right (see above), and do that consistently, your work will speak for itself, and your network will grow as you acquire new contracts through referrals. I like to think of this concept as “quiet networking”: do a good job, be friendly and open minded, don’t cause any dramas, and people will be happy to connect with you."
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html#say-yes",
    "href": "posts/2024-06_Lessons/index.html#say-yes",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "Say yes 👍",
    "text": "Say yes 👍\nThis leads nicely into the next point: say YES to things. What I mean by that is be open-minded to new work, and don’t be afraid to branch out a bit and step outside of your comfort zone. In doing so you will learn a lot, build up your capacity and your network, and this will bring in more work and diversify your business.\nTo give an example, when I finished working at the JRC in 2020 I had a mixed bag of skills in policy, statistics and research. One of my first big contracts was to build an R package for composite indicators. At that time, my programming skills in R were actually fairly mediocre at best, and I could perhaps have turned the work down on the basis of “not being a programmer”. But looking at it from another angle, it was a great opportunity to learn to be a programmer and get paid to do it! A few years later I would now consider myself to be a fairly expert-level programmer in R, and this has led into all kind of interesting new projects.\n\n\n\nGet out there and say yes!\n\n\nSimilarly, in 2023 I branched out into building web-apps using Shiny, based on some dabbling that I had done a year or two earlier. I took on two big contracts to build Shiny apps and learned an enormous amount about Shiny, HTML, CSS in the process. Again, this diversified my business, but was also a really interesting new branch of work.\nThe point here is not to box yourself in to one narrowly-defined profession. You can learn new things. Learning new things is fun, and it pays back to you!"
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html#say-no",
    "href": "posts/2024-06_Lessons/index.html#say-no",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "Say no 👎",
    "text": "Say no 👎\nOK, so say yes to things, except when you have to say no! But when to say no? As an optimist, I don’t often say no to things, but sometimes this is important. I’ll give two examples here.\n\n\n\nNo way\n\n\nThe first is to enforce contractual boundaries when it is necessary. Most freelance contracts will come with a set of deliverables attached, which are used to estimate the number of working days necessary, and in turn the cost of the work (AKA, what you get paid). Just as it is important for you to make sure you deliver the deliverables (the hint is in the name), it is important for your client to also respect that strictly speaking, you are contracted for those deliverables, and not for any other tasks that they might dream up later on.\nWith that said, it is very hard to anticipate exactly what needs doing in a contract, so leaving a bit of wiggle room on both sides is important. Don’t nit-pick over small tasks that were not explicitly specified, otherwise your client will nit-pick back at you. But if large tasks appear that were not specified in the contract, it may be time to put your foot down. Always do this politely: kindly remind the client what was specified in the contract, be clear that you are flexible up to a point, but the task is too large to incorporate in the present contract, and be proactive to discuss how to get the extra tasks done (e.g. by extending or drawing up a new contract). You can also consider swapping one deliverable for another: the aim is to seek a way forward that gets the work done for the client, but also gets you paid fairly for your time.\nIt’s important to enforce these boundaries because if you don’t, they are likely to happen more often, but also because it is perfectly fair and reasonable. And in most cases, it is probably just that the client has forgotten what the deliverables were, and just needs a friendly reminder.\nThe other NO example is saying no to new contracts. This one deserves a point on its own, so I’ll see you in the next paragraph…"
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html#find-your-balance",
    "href": "posts/2024-06_Lessons/index.html#find-your-balance",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "Find your balance 🧘",
    "text": "Find your balance 🧘\nHello again! As I was saying, sometimes you have to turn down contracts.\nIf you work for a big company or organisation, it’s your manager’s job to make sure you have enough work to keep you busy, but not so much that you you are overloaded. Of course, you also have a say in the matter, but often it is possible to redistribute work to other team members if you become overloaded. Not so when you are working freelance! Unless you team up with other people, you are responsible for 100% of the deliverables in contracts you have open, in sickness and in health.\nThis means you have to be very careful not to take on more work than you can manage, otherwise you will miss deadlines and/or deliver sub-par work. And you need to build in a little wiggle room for possible illness and other unforeseen delays.\n\n\n\nBalance is important\n\n\nIt takes experience to know how much work you can take on and still be comfortable. It’s all about finding your balance, your own sweet spot, and if you feel you are overdoing it, don’t be afraid to turn down work - in any case, often clients will be very reasonable and will offer you more work down the line if you want it."
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html#know-your-worth",
    "href": "posts/2024-06_Lessons/index.html#know-your-worth",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "Know your worth 💸",
    "text": "Know your worth 💸\nAnother thing that only comes with experience is understanding how much you can charge for your work. Especially at the beginning, it is easy to underestimate or even overestimate your potential daily rate if you haven’t had to specify that before. Keep in mind that some large organisations have fixed rates for consultants, whereas for others, it is completely flexible. Also, daily rates vary a lot based on the type of work and where you are.\n\nIf you know other people working on similar projects, don’t be shy to ask around a bit to get a ballpark figure. While you might be tempted to set your rate lower at the beginning, just keep in mind that some projects can run over several years, and it can be tricky to change your daily rate with an existing client once it has been established, so better to get it in the right range from the outset! Don’t forget that your daily rate has to include things like admin, contract management, sorting out IT problems and other niggling bits and pieces that you might not consider at first glance.\nKeep in mind though that although daily rate is important, what really matters is the total that you get paid, and the amount of your professional time you have to spend on it. Some organisations may have a lower fixed daily rate, but are prepared to allocate more days to make up for that."
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html#spot-the-synergies",
    "href": "posts/2024-06_Lessons/index.html#spot-the-synergies",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "Spot the synergies 🍻",
    "text": "Spot the synergies 🍻\n“Synergies” is a dangerous word because, at least for me, it sounds like a vacuous buzzword that is carelessly tossed around in management meetings to score a few points, without necessarily having a lot of substance behind it.\n\nNevertheless, synergies are a real and useful thing and quite handy if you can spot them. As a freelancer, if you can work on separate projects that involve similar types of work, you can save yourself time and potentially improve your work for your clients on both projects. In my work, for example, I have often been able to reuse parts of my code, data pipelines and workflows for similar projects. With coding especially, if you generalise the code you are writing, it takes a little more time the first time around, but pays dividends down the line (see automation later).\nTo give a couple of examples, when developing the COINr package, I was simultaneously building a similar composite indicator R package for WIPO to calculate the Global Innovation Index. Although the two packages are distinct and have different aims, I have constantly passed concepts and code between the two, for the enrichment of both. Similarly, when working on Shiny apps I developed a library of accessory functions which make life easier for a GUI to work with COINr - this library was shared between the two projects and expanded on both.\nIn general, synergies can allow you to deliver better-quality work in a shorter time, which obviously tends to make your clients quite happy."
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html#time-or-deliverables",
    "href": "posts/2024-06_Lessons/index.html#time-or-deliverables",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "Time or deliverables? 🤷‍♂️",
    "text": "Time or deliverables? 🤷‍♂️\nI’ve worked on two main types of contracts: those that are focused on deliverables, and those that are focused on time. It’s important that everyone is clear which type of contract you are working on.\nIn practice all contracts come with deliverables. But in some cases, the client deliberately leaves the deliverables fairly vague and contracts you for e.g. 20 days of your professional time, in which you can negotiate work between you. When the 20 days are up, your contract is finished: in other words, the contract is considered complete based on the number of days, and not really on the deliverables.\nIn a deliverables-focused contract, you agree to a set of deliverables, and a cost. The contract may or may not specify the number of working days, but in either case you are required to complete the deliverables to complete the contract, and the client is not so interested in how long you worked, as long as everything gets done.\n\n\n\nContracts are important but hard to get right\n\n\nThere are pros and cons of both of these. In the first case you have the security that after working 20 days you get paid for exactly 20 days of work. The second case is more risky in that there is the chance your work takes longer than your cost estimate, and you still have to deliver. However, it goes both ways: if you are efficient, it may also possible to complete the work more quickly, which effectively increases your daily rate. Obviously you still have to ensure to deliver high-quality work - if you rush it and do a sloppy job then you may damage the relationship with your client and consequently, your all-important professional reputation.\nWhere you definitely don’t want to be is in a mix between these two contract types, which is why everything needs to be clear up front. The danger is (from the contractor perspective) that you may think you are on a time-focused contract, and accept all kinds of arbitrary work from the client, but after your allocated days have been spent, the client suddenly starts demanding the completion of contract deliverables. Likewise you may have completed all deliverables efficiently on an ostensibly deliverable-focused contract, after which the client wants to use any remaining working days on other projects.\nIt has to be one or the other: either stick the the deliverables, or the time, but don’t switch between them. This is something that is worth clarifying explicitly with your client when drawing up the contract."
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html#manage-objectives-and-expectations",
    "href": "posts/2024-06_Lessons/index.html#manage-objectives-and-expectations",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "Manage objectives and expectations 💘",
    "text": "Manage objectives and expectations 💘\nIf you are doing technical work for your clients (e.g. data science), sometimes your client may not have a perfectly clear idea of what they want from you. This is perfectly normal because they don’t know everything about what you can and can’t do, what tools and options are out there, and their expertise is often in a different field to yours. This is why it is worth spending some time nailing down the objectives and expected outcomes of the project right at the beginning.\n\n\n\nAim carefully\n\n\nOne thing that has worked for me is to draw up my own proposal which states what it is I propose to do under the contract. This sets out your understanding of the work, and ensures you are on the same page as them from the outset. It also acts as a reference in case things go astray at any point.\nEqually important is to check in regularly with the client, even if they don’t specifically request it. If not, you might spend a couple of months working hard on your deliverables, only to find there was a fundamental misunderstanding between you and the client, and you have gone spectacularly off track.\nIt all comes back to good communication: agreeing and clearly stating objectives and outcomes, and sharing and discussing results as the project evolves."
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html#automation-and-outsourcing",
    "href": "posts/2024-06_Lessons/index.html#automation-and-outsourcing",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "Automation and outsourcing 💻",
    "text": "Automation and outsourcing 💻\nOne of the nice things about freelancing is that you are usually free to manage your tools and methods as you like. This opens up a lot of possibilities for automation and time-saving.\nOn the admin side of things (yes unfortunately freelancing comes with a fair dose of admin) get a system set up early on for tracking your time, active and upcoming contracts, invoices and payments, and so on. I did this one in Excel and it took some iterations to get something that works well for my needs. However, once it’s up and running, it saves a lot of time and is essential in keeping track of your time and money. Most likely you can find apps that deal with this as well, if you prefer to go down that route.\n\n\n\nIt is the year 2000…\n\n\nFor handling tax, I’d recommend finding a good accountant. It costs money, but unless you love wading through tax regulations, it’s money very well spent. Actually here in Italy, almost no one dares to do their own taxes because it’s a minefield of obscure rules, arcane jargon and traps that you can unwittingly fall into and get busted for several years in the future, so an accountant is not even optional in practice.\nOverall, consider that your time is valuable, and if you spend it on things that are not your profession, you might do well to outsource that work, unless you specifically want to branch out into that area.\nGetting your files well organised also helps a lot, and using cloud storage such as OneDrive allows you to access your work from anywhere, even your phone, and also acts as an all-important backup in case your laptop has a meltdown.\nFinally, if you do much coding, you can really optimise things by generalising your code so that it can be reused easily in other projects. For example, I developed the iCOINr package which contains functions for generating interactive plots related to composite indicators which I generated for one project and reused in several others. I also have libraries of utility functions which quickly perform common tasks, which I can can import into any relevant project."
  },
  {
    "objectID": "posts/2024-06_Lessons/index.html#finally-look-after-yourself",
    "href": "posts/2024-06_Lessons/index.html#finally-look-after-yourself",
    "title": "12 Lessons learned from four years of freelancing",
    "section": "Finally: Look after yourself 🤗",
    "text": "Finally: Look after yourself 🤗\nLast but by far not the least: take good care of yourself. You are your number one resource and asset, so treat yourself as such.\n\n\n\n\n\nThat means: don’t work yourself to death. Make sure you stay reasonably fit and healthy. Eat well. Get outside. Meet people in person where possible, rather than online. Take holidays, enjoy doing whatever it is that you like to do in life!\nThis advice might sound a little glib, but when you work for yourself it easy to overwork. One reason is that as we have discussed, it’s hard to get exactly the right amount of work, and often the error is on the “slightly too much” side of things. But also, since your income is linked to the amount of work that you do, it is tempting to work extra in the evenings to tick off some tasks and earn some extra money. Although I’ve been guilty of this from time to time, I don’t think it is a good idea in general and I try to avoid it. Life is about much more than work, and even from a work point of view you are not doing yourself any favours by running yourself into the ground.\nSpeaking of which, I’m now off for a bike ride :)"
  },
  {
    "objectID": "posts/2024-03-01_ESI_release/index.html",
    "href": "posts/2024-03-01_ESI_release/index.html",
    "title": "Update and release of the European Skills Index",
    "section": "",
    "text": "Happy to say that the latest edition of the European Skills Index (ESI) was launched yesterday. I have worked on this project with Hedvig Norlén for the past few months to update the ESI with the latest data. As with all the composite indicators that I work on these days, I am trying to create a data pipeline that is as reproducible and automated as possible, and to minimise the chance of errors. Here’s what I did in this case."
  },
  {
    "objectID": "posts/2024-03-01_ESI_release/index.html#data-collection",
    "href": "posts/2024-03-01_ESI_release/index.html#data-collection",
    "title": "Update and release of the European Skills Index",
    "section": "Data collection",
    "text": "Data collection\nThe ESI is composed of 15 indicators. Of these, 11 come from Eurostat, which means we can use the rather handy Eurostat package which is an easy R interface for the Eurostat API. This means the data can be directly imported from Eurostat’s database into R, without any intermediate downloads. It took me a while to get the exact indicator codes for each indicator, since these were not always clear from the previous releases, and in some cases we had to do some cross checking with previous data to figure out exactly what indicators were used. In the end the data was downloaded like this:\n\nlibrary(eurostat)\n\n# clean cache if necessary\neurostat::clean_eurostat_cache()\n\n# Ratio of pupils and students to teachers and academic staff at the pre-primary education level (ISCED11 level 0, 3 years to the start of primary education, Eurostat code educ_uoe_perp04)\nPTRatio &lt;- get_eurostat(\"educ_uoe_perp04\", time_format = \"num\", filters = list(isced11 = \"ED02\"))\n\n# Share of population aged 15-64 with at least upper secondary education (ISCED11 level 3-8, Eurostat code edat_lfse_03)\nSecEd &lt;- get_eurostat(\"edat_lfse_03\", time_format = \"num\", filters = list(sex = \"T\", age = \"Y15-64\", isced11 = \"ED3-8\"))\n\n# Share of population aged 25-64 who stated that they received formal or non-formal education or training in the four weeks preceding the survey.\nRecTrain = get_eurostat(\"trng_lfse_01\", time_format = \"num\", filters = list(sex = \"T\", age = \"Y25-64\"))\n\n# Share of the population at ISCED11 level 3 attending vocational training (Eurostat code educ_uoe_enra13)\nVET &lt;- get_eurostat(\"educ_uoe_enra13\", time_format = \"num\", filters = list(isced11 = \"ED35\"))\n\n# Share of individuals who performed more than one activity in all skills domain (information, communication, problem-solving, software) Eurostat code isoc_sk_dskl_i\nDigiSkill &lt;- get_eurostat(\"isoc_sk_dskl_i\", time_format = \"num\", filters = list(indic_is= \"I_DSK_AB\", unit = \"PC_IND\", ind_type = \"IND_TOTAL\"))\n\nI have cut this short to avoid a very long code chunk, but you get the idea. This process results in a set of data frames which I wrap as a list. In each case I download as much historical data as possible. The advantage of using the API, among others, is that the data arrives clean as a whistle and there is very little to do to get it in the format you need.\nNext I brought in the other four indicators which were collected by colleagues at Fondazione Giacomo Brodolini and passed as Excel files. I wanted to make the methodology behind each indicator as reproducible as possible, so I put each of these indicators in a separate folder, each with its own R script which reads the untouched data and outputs a csv file in a standardised format with a consistent naming convention.\n\n\n\nGeneral data sourcing sourcing approach\n\n\nFinally in my central R script I import the four non-API indicators and merge them with the data collected via APIs. This results in a cleaned and updated data set spanning all 31 countries and going from 2015-2022. In fact some indicators go back further but the data gets too patchy. Still, 8 index years is already good!"
  },
  {
    "objectID": "posts/2024-03-01_ESI_release/index.html#data-validation",
    "href": "posts/2024-03-01_ESI_release/index.html#data-validation",
    "title": "Update and release of the European Skills Index",
    "section": "Data validation",
    "text": "Data validation\nIt’s super-important to be sure that the data you collected is the correct data. Otherwise you risk someone spotting this after the launch and then… well, let’s just say it’s better to check thoroughly first.\nThe last round of data collection for the ESI (before I was involved) collected data from 2020 all the way back to before 2015. This meant I had several years of data to compare for each country and indicator. Using R (of course) I compared every data point (year-country-indicator combination) and calculated the differences between the two data sets. This identified a few issues:\n\nIndicator codes which I had slightly mistaken\nManual data collection which needed some adjustments\nCases where the previous methodology was not very clearly described\n\nAfter some iterations we were able to fix all of these problems. The result here was a completely clean and validated data set. We noted that as expected, some time series for certain countries had been revised by Eurostat, creating minor differences here and there."
  },
  {
    "objectID": "posts/2024-03-01_ESI_release/index.html#replicate-methodology",
    "href": "posts/2024-03-01_ESI_release/index.html#replicate-methodology",
    "title": "Update and release of the European Skills Index",
    "section": "Replicate methodology",
    "text": "Replicate methodology\nNext step was to be able to replicate the ESI methodology precisely in R. Previous editions of the ESI had been calculated with a rather nice Excel tool, but this was a bit stretched to handle multiple years of data, and is hard to follow. Instead, I recreated the methodology using my ever-evolving COINr package in R. COINr is already highly flexible and can handle most composite indicator tasks that you throw at it, but I had to modify the package to allow linear time-series imputation for missing data. Then, COINr was used for the following steps:\n\nWith the validated data set, fill in missing values using linear time-series imputation, or else the latest available data point.\nNormalise using a “distance to frontier” approach.\nAggregate using a mixture of arithmetic and geometric weighted means.\n\nTo validate the methodology, I took the data set used in the previous edition and used R to calculate the results, then cross-checked each data point at each step against the previous results. COINr was able to exactly reproduce the results."
  },
  {
    "objectID": "posts/2024-03-01_ESI_release/index.html#calculate-index",
    "href": "posts/2024-03-01_ESI_release/index.html#calculate-index",
    "title": "Update and release of the European Skills Index",
    "section": "Calculate index",
    "text": "Calculate index\nAt this point I have both a validated data set and validated methodology. Since COINr is flexible to the the number of countries and the number of years in the data set, I simply feed the validated data into the same commands that were used when replicating the results in the previous step. The high-level commands are in fact very simple:\n\nlibrary(COINr)\n\n# assemble coin\nESI &lt;- new_coin(iData, iMeta, split_to = \"all\", level_names = c(\"Indicator\", \"Sub-pillar\", \"Pillar\", \"Index\"),\n                retain_all_uCodes_on_split = TRUE)\n\n# impute data with linear interpolation\nESI &lt;- Impute(ESI, dset = \"Raw\", f_i = \"impute_panel\", f_i_para = list(imp_type = \"linear\"))\n\n# normalise with distance to frontier normalisation using params in iMeta\nESI &lt;- Normalise(ESI, dset = \"Imputed\", \n                 global_specs = list(f_n = \"n_goalposts\", f_n_para = \"use_iMeta\"),\n                 global = FALSE)\n\n# aggregate using mixture of arithmetic/geometric mean\nESI &lt;- Aggregate(ESI, dset = \"Normalised\", f_ag = c(\"a_amean\", \"a_amean\", \"a_gmean\"))\n\nOf course, this hides a lot of complexity that is going on behind the scenes. But this ought to be the point of a good R package, IMHO :)\nAt this point I can have a peek at the results.\n\n\n\nSample of the results table\n\n\nWhat we end up with is a results table for each year, from 2015-2022."
  },
  {
    "objectID": "posts/2024-03-01_ESI_release/index.html#outputs-and-exports",
    "href": "posts/2024-03-01_ESI_release/index.html#outputs-and-exports",
    "title": "Update and release of the European Skills Index",
    "section": "Outputs and exports",
    "text": "Outputs and exports\nLast thing to do is to export to the various formats needed. In this case this consisted of:\n\nResults for all years exported as a formatted Excel workbook.\nLong-form csv of results to pass to web developers for the online portal.\nAutomatically generated texts describing each country.\n\nThe texts were an interesting one. Rather than manually writing 31 pieces of text, I wrote a template which was populated by variables which were calculated for each country. This was a little tricky because when describing changes we may need different sentence constructions for positive, negative, or no changes, for instance. The output template looked like this:\n\nglue::glue(\n    \"# {country_name}\n    \n    {country_name} ranked {to_ord(ESI_rank)} of 31 countries in the European Skills Index in 2024, with a total score of {ESI_score}.  At the pillar level, it ranked {skdev_rank} in Skills Development (score: {skdev_score}), {skact_rank} in Skills Activation (score: {skact_score}) and {skmat_rank} in Skills Matching (score: {skmat_score}).\n    \n    The highest-ranking indicators of {country_name}, in comparison with other countries, are {itop1_name} ({itop1_val}, rank {itop1_rank}) and {itop2_name} ({itop2_val}, rank {itop2_rank}). On the other hand, its weakest indicators are {ibot1_name} ({ibot1_val}, rank {ibot1_rank}) and {ibot2_name} ({ibot2_val}, rank {ibot2_rank}).\n    \n    Over the last seven years the overall rank of {country_name} has {ESI_rankchange_snippet}. In that time, its overall score has {ESI_scorechange_snippet}. {indicator_increases_snippet}. {indicator_decreases_snippet}.\n    \nNote that these figures may refer to imputed data points.\n\n    \"\n  )\n\nOf course this excludes quite a lot of code previously where I calculate the values and text snippets for each country. To get the output I just loop this template over all countries, then glue everything together into one big text file.\nLast thing was, we anyway pasted the latest data into the Excel tool mentioned previously to calculate the results in parallel in Excel (which is for internal Cedefop use). Then, we wrote the methodological report, which can be found here. A couple of figures follow.\n\n\n\nChanges in rank over 2017-2024\n\n\n\n\n\nScores of top and bottom countries in index, pillar and sub-pillars\n\n\n\n\n\nHungary has seen a big increase in its ESI score relative to other countries, having increased 6 rank places to 13th place in 2024. One reason for this is big drops in long-term unemployment and underemployed part-time workers.\n\n\n\n\n\nReading, maths and science scores over PISA waves 2015, 2018 and 2022."
  },
  {
    "objectID": "posts/2024-03-01_ESI_release/index.html#thoughts",
    "href": "posts/2024-03-01_ESI_release/index.html#thoughts",
    "title": "Update and release of the European Skills Index",
    "section": "Thoughts",
    "text": "Thoughts\nI think this is getting towards a fairly good template for building a composite indicator. Rather than doing all of this in one giant script, the work was split into five Quarto (R) notebooks:\n\nData collection\nData validation\nMethodology replication\nIndex calculation\nExports\n\nI strategically save the outputs of each to files, so that I don’t have to re-run the whole thing each time I make a change. This seems like a good strategy. Still fine-tuning things though!\nThe ESI was good to work on because it is a well-established methodology, which allows to focus a bit on the data pipeline and technical implementation. I will definitely be reusing some of the ideas here in future work."
  },
  {
    "objectID": "posts/2023-12-01_FINDApp_update/index.html",
    "href": "posts/2023-12-01_FINDApp_update/index.html",
    "title": "Updates on the FIND composite indicators app",
    "section": "",
    "text": "The Foundation for Innovative New Diagnostics (FIND) is a non-profit organisation which “seeks to ensure equitable access to reliable (medical) diagnosis around the world. Early in 2023 I began working with them to develop a Shiny app which can build composite indicators to help them identify in which countries to direct their resources.\nAfter much hard work and collaboration with talented people, we are nearly ready to release the app. Although the app was developed as a tool for use within FIND, we should be able to release an open-source version of it, which can be installed as an R package, but run as an app.\nThe app is essentially a front end for the COINr package, but also has a number of modifications and tries to strike a balance between flexibility and not overwhelming the user with too many options. Its features include:\n\nUpload of any data set and index structure\nScreening units by data availability\nImputation of missing data\nOutlier treatment\nNormalisation and aggregation\nStats, maps, bar and bubble charts\nUnit profiles\nGlobal sensitivity analysis and reweighting\n\nI leave a couple of screenshots here, and hopefully the app will be made freely available in the near future!"
  },
  {
    "objectID": "posts/2023-11-17_EIS_bid/index.html",
    "href": "posts/2023-11-17_EIS_bid/index.html",
    "title": "European Innovation Scoreboard",
    "section": "",
    "text": "I have done a bit of writing of project bids in the past but this was the first time that I got more heavily involved. As part of a really talented team (who I won’t mention at this point for privacy reasons) I helped to put together a bid to update and maintain the European Innovation Scoreboard.\nThis is a scoreboard and composite indicator owned by DG-RTD that measures innovation in European countries, and also looks at the sub-national level in the form of the Regional Innovation Scoreboard. The call for tender is a fairly big one and involves a few years of work. My role as usual would be to do the composite indicator calculations.\nOf course we all have our fingers crossed that our bid might be successful. For my part, I have strongly advocated for reproducible development of composite indicators and good data science practice (e.g. programmatic approach, unit testing, use of collaborative platforms like GitHub and so on).\nOther than that it was an interesting experience in compiling a big bid and working as a consortium. It was a lot of work, all the more so because it is prospective. But I think the result was pretty good. So I think we have a fighting chance to make it."
  },
  {
    "objectID": "posts/2023-10-10_KSA_workshop/index.html",
    "href": "posts/2023-10-10_KSA_workshop/index.html",
    "title": "2023 Wrap Up",
    "section": "",
    "text": "Work on the Quality Infrastructure for Sustainable Development Index (better written as its acronym QI4SD Index) is warming up again. And not many places are warmer than Riyadh in Saudi Arabia!\nOn 3-4 October I was invited to Riyadh as part of the UNIDO-led team working on the QI4SD Index to present the index methodology to the Saudi Accreditation Center and the Saudi Standards, Metrology and Quality Organization. We spent two days of very interesting and useful discussions, working on ways to improve the index and examining the QI and sustainable development context of Saudi Arabia.\nRiyadh was also a fascinating city to visit and to experience some Saudi culture and hospitality."
  },
  {
    "objectID": "posts/2023-09-08_EDGAR_booklet/index.html",
    "href": "posts/2023-09-08_EDGAR_booklet/index.html",
    "title": "Global emissions data set published",
    "section": "",
    "text": "In September 2023, as part of my work as a data scientist at the Joint Research Centre, I contributed to the GHG emissions of all world countries booklet.\nThe booklet is an annual output of the EDGAR database team (of which I am a member) and provides emissions time series from 1970 until 2022 for GHGs for all countries and for all anthropogenic sectors. EDGAR is used around the world as a highly-detailed, consistent and high-quality data source for research and policy making.\nMy particular role in this endeavour was calculating uncertainties on emissions estimates. This is a fairly complex procedure given the size and complexity of the database, and also due to issues such as skewed distributions and correlations. To do this I built a custom R package which is live-linked to the EDGAR database using SQL drivers. Uncertainties can be explored, visualised and exported internally via a Shiny app.\nI’m quite proud of my contributions so far to the EDGAR project (although these are mostly internal), and pleased to be a part of a team that contributes significantly to climate change research and policy."
  },
  {
    "objectID": "posts/2023-01-11_Teams/index.html",
    "href": "posts/2023-01-11_Teams/index.html",
    "title": "I don’t like MS Teams",
    "section": "",
    "text": "Sorry, this has to be said to help with my emotional well being because Teams has driven me to the depths of despair and this is the only way I can think of to begin some kind of cathartic healing process.\n(disclaimer: please take the following rant in the humorous way that it is intended) :innocent:\nI work as a freelance consultant for multiple organisations. I also have my own Microsoft account. Every time I am invited to collaborate on a project with a new organisation I have sleepless nights because I know what’s coming. It’s only a matter of time. The dreaded email. Those awful words.\n“Let’s connect on Teams”.\nSo yet again, I have to go through the same process. Click on the link emailed to me. MS assumes I want to join with a different account that I have used for a different org and tries to sign me in with that org. Fails. Offers no way to sign out and to join the actual org that invited me. Sometimes I have managed to sign out, MS says I’m signed out. Click the link again, no actually I wasn’t signed out because EXACTLY the same thing happens. So then I open link in incognito mode. About 50% of the time I manage to actually join the meeting, the other half not, with no explanation of what the problem actually is and we end up moving to Zoom or WebEx. Still got no idea now how to join a meeting without opening in incognito mode.\nBut that’s just for a one off meeting. If I want to actually join a “Team”, as a guest, it’s 10x worse. Asked to create an account. But then it starts harping on about work vs personal emails. I end up trying from all kinds of different email addresses, emailing the clients back and forth. Usually face all kinds of dead ends. It asks me to install some stupid app. And/or sends verification codes that don’t work, asks for my phone number, and after I’ve handed over all my personal information, my bank details, DNA samples, embarrassing pictures and whatever else it still gleefully denies access.\nOn some occasions I actually make it into Teams but I can’t see the “Team” I was invited to. Cue more frantic emails with client, client consulting IT department, IT department stumped. After a few hours perhaps we finally get in through randomly exploring every possible combination of emails, settings and any other explanatory variables that come to mind. Having finally cracked the safe, client asks me to look at document X. Discover that I don’t have permission to look at the document (despite it being in the file repository of the team that I’m a member of). Everyone just gives up and moves back to email.\nI can only imagine that Teams was invented as some kind of purgatory for our sins. If that’s the case, I can only say that when I die, I will die with a pure soul because I have atoned one hundredfold for the sins of my life, in tears shed and time lost over Microsoft Teams. And what’s more, I will die happy in the knowledge that in the afterlife, whatever it may be, there will be no Teams. I hope."
  },
  {
    "objectID": "posts/2022-12-05_SAMOweb_plus/index.html",
    "href": "posts/2022-12-05_SAMOweb_plus/index.html",
    "title": "New projects plus website",
    "section": "",
    "text": "It’s difficult to make a post about everything, so here’s a summary of some recent things."
  },
  {
    "objectID": "posts/2022-12-05_SAMOweb_plus/index.html#samo-website",
    "href": "posts/2022-12-05_SAMOweb_plus/index.html#samo-website",
    "title": "New projects plus website",
    "section": "SAMO Website",
    "text": "SAMO Website\nFirst, I just launched today the group website for the Sensitivity Analysis of Model Output (SAMO) group. I started this over a year ago, but finally decided to get it properly over the finish line. SAMO is an international group of researchers and academics working on sensitivity analysis in different fields, and organises the SAMO conference every three years. I’ve been to the conference several times and am a member of the SAMO board. If you want to know more, hmm where can you go to find out? Oh yes, the new website of course!"
  },
  {
    "objectID": "posts/2022-12-05_SAMOweb_plus/index.html#projects",
    "href": "posts/2022-12-05_SAMOweb_plus/index.html#projects",
    "title": "New projects plus website",
    "section": "Projects",
    "text": "Projects\nThings are definitely starting to get busy now. Which is why I decided to finish the SAMO website before I have no time for it!\nI’ve been continuing my work with the great people at the Global Innovation Index, implementing new features, improving test coverage, and generally making the package more useful and robust. I’m also just about to start an audit of the Vietnam regional innovation index (a kind of spinoff of the GII), which is a new index, yet to be launched.\nVery recently, as mentioned elsewhere, I have started working on a very interesting project: a Shiny App to wrap the COINr package in for the good people at FIND. I’ve been preparing the mockups. This could be a very interesting app for many people but will also be non-trivial to develop. I am working with some very skilled developers though so it is looking promising.\nElsewhere several contracts are (probably) about to start. Some work for the JRC on impact assessments, some other work for the JRC on air pollution data analysis, and an index for the UNHCR in Guatemala. Can’t give too many details yet but these things should happen. I also have some other (less certain) stuff in the pipeline.\nSo, I will be very busy in the coming months, working on all kinds of interesting topics!"
  },
  {
    "objectID": "posts/2022-09-14_Peer_Review/index.html",
    "href": "posts/2022-09-14_Peer_Review/index.html",
    "title": "Open peer review: a better way?",
    "section": "",
    "text": "If you have worked in academia for any length of time at all, you will know a bit about peer review, and will most likely have had some frustrating experiences along the way.\nBut for non-academics, what is peer review anyway? Well, first, it’s necessary to explain that much of an academic’s worth is measured by the number (and to some extent, the quality) of academic publications. A publication is basically a report of some research done by the academic, usually of around 6000-10000 words. The “manuscript” (yes for some reason we have to pretend we’re in Ancient Egypt) is submitted to an appropriate journal, it is reviewed by “peer review”, and then if it is accepted, it is published, and the academic and his/her colleagues get a juicy publication point and accompanying citations to beef up their reputation.\nMore or less, the process of publication looks like this:\nThis is a simplified version of the process, and the review part typically takes some months, but may take years. Importantly, the large majority of reviews are done confidentially, so only the editor, the reviewers and the author, see what happened. Often the reviews are blind (the reviewers see the names of the authors but not the other way around), or double-blind (both authors and reviewers don’t know the names of each other, everything is anonymised). I’ll return to this point."
  },
  {
    "objectID": "posts/2022-09-14_Peer_Review/index.html#hostile-reviewers",
    "href": "posts/2022-09-14_Peer_Review/index.html#hostile-reviewers",
    "title": "Open peer review: a better way?",
    "section": "Hostile reviewers",
    "text": "Hostile reviewers\nEvery academic has come up against hostile reviewers. The review process is not meant to be easy, but a surprising number of reviewers seem to nit-pick to an extent that seems to go beyond what is reasonable. They may insist on extensive but unnecessary modifications, or are constantly unsatisfied with revisions. In an ideal world the editor would spot these over-zealous reviewers, but editors are busy. Sometimes such reviews may be due to the fact that the reviewer is from a competing institution, and therefore there is little reason (apart from professionalism and honesty) for them to play fair. This problem is enabled by the fact that the review process is done behind closed doors, so there are virtually no consequences if reviewers don’t behave themselves.\nBlind and double-blind reviews should help vindictive attacks between rivals in theory, but in practice many research fields are small enough that it is often fairly obvious, even without names, to know who wrote the paper (if you are the reviewer) and to know who the reviewers are (if you are the author)."
  },
  {
    "objectID": "posts/2022-09-14_Peer_Review/index.html#friendly-reviewers",
    "href": "posts/2022-09-14_Peer_Review/index.html#friendly-reviewers",
    "title": "Open peer review: a better way?",
    "section": "Friendly reviewers",
    "text": "Friendly reviewers\nAt the other end of the spectrum are the reviewers who are your mates. To save editor time (and ostensibly to find suitable reviewers), in many journals authors are allowed to suggest reviewers for their paper. In theory, reviewers are not supposed to have any close connection with the authors, but in practice, editors have to deal with many papers and reviewers are scarce. So often, the authors’ recommended reviewers end up reviewing the paper.\nClearly the problem is that if the authors are not playing cricket, so to speak, they can just recommend their friends as reviewers. Depending on the level of integrity, these reviewers may give an overly-soft review or even accept the paper outright with no modifications. The result is that the paper hasn’t really been through a real peer review. Again, this problem is facilitated by the fact that the review is not public."
  },
  {
    "objectID": "posts/2022-09-14_Peer_Review/index.html#publication-for-citations",
    "href": "posts/2022-09-14_Peer_Review/index.html#publication-for-citations",
    "title": "Open peer review: a better way?",
    "section": "Publication for citations",
    "text": "Publication for citations\nA fairly innocent-sounding comment is when a reviewer recommends that the author make some further citations to specific papers, a.k.a. “relevant literature”, possibly to “improve the context of the work”. The catch is that all of these citations happen to be the reviewer’s papers, or else those of their friends. This puts the authors in an awkward position: they can cite the proposed papers, which is easy to do, and that will appease the reviewer and bring them closer to publication. Alternatively, they could contest the recommended citations, but this will likely delay the process and could aggravate the reviewer. By far the easiest option is to just cite the papers - is it worth risking possibly months of delays? But it is irksome to have to “pay passage” to the reviewer in this way, and is obviously not ethical."
  },
  {
    "objectID": "posts/2022-09-14_Peer_Review/index.html#lazy-reviews",
    "href": "posts/2022-09-14_Peer_Review/index.html#lazy-reviews",
    "title": "Open peer review: a better way?",
    "section": "Lazy reviews",
    "text": "Lazy reviews\nA proper review of a paper takes time, sometimes rather a long time. Apart from anything else, many research papers are complex and take time to understand, even for experts. You should carefully read the whole paper and try to spot any flaws in the methodology, mistakes in equations, and make sure it is written clearly and concisely. You should make as many suggestions as possible to improve the paper (without being nit-picky - see above), and be prepared to spend time communicating with the authors after successive revisions. Sometimes you have to check the cited work to make sure the citations actually support the statements made in the paper.\nSince reviews are unpaid and uncredited, it is easy for them to fall low on the priority list of the reviewers’ tasks, especially in busy periods. This can sometimes lead to very lazy reviews, where a reviewer writes a short review to simply tick it off the list. In one case, a reviewer of my paper simply copied the abstract of the paper (presenting it as his/her review) and added one vague sentence saying that it wasn’t good enough. It was clear that the reviewer had not even read the paper and had probably spent five minutes compiling the “review”.\nThe issue is that of course it is far easier to reject a paper on vague grounds, rather than attempt to read it properly, understand it, and offer constructive comments."
  },
  {
    "objectID": "posts/2022-09-14_Peer_Review/index.html#the-never-ending-story",
    "href": "posts/2022-09-14_Peer_Review/index.html#the-never-ending-story",
    "title": "Open peer review: a better way?",
    "section": "The never-ending story",
    "text": "The never-ending story\nReviewing takes time, as we have seen. But there should be reasonable limits. To describe a personal example, the research for one paper of mine was completed in 2014. We had a hard time finding a suitable home for the work because it was a fusion of two fields (sensitivity analysis and econometrics). At one point it was stuck in one journal for a whole year with no response from the editor. It was rejected from a few other journals because of the topic and other reasons. Finally we submitted it to another journal, where it took two and a half years to be reviewed. In total, it took us seven years to get the paper published.\nI’m not claiming that the paper had a divine right to publication, and in some cases reviewers had made reasonable points for rejecting the paper, which we subsequently addressed. But if research takes years to review, that is a problem for the authors and for the wider research field because there is such a huge lag in making new work visible. It means that published literature doesn’t represent the state of the art, but rather the state of the art a couple of years ago, or more."
  },
  {
    "objectID": "posts/2022-03-01_OccamsRazor/index.html",
    "href": "posts/2022-03-01_OccamsRazor/index.html",
    "title": "Occam’s Razor",
    "section": "",
    "text": "So far in my few posts in this blog I’ve been writing about indicators. Actually I only started working in indicators around 2015. I found that the topic suits me because it is a blend of statistics/data analysis and qualitative work (writing, conceptualising and so on), and I like things that are not too narrowly focused.\nBefore that (and still now) I was/am a researcher in sensitivity and uncertainty analysis. Uncertainty analysis is understanding how uncertainties in model inputs affect the results, and sensitivity analysis is a more detailed breakdown of which particular model inputs/assumptions are causing the most uncertainty, and by how much.\nThese topics, combined with some years of experience working in the European Commission, also led me to be generally interested in modelling. I had direct experience building engineering and biomechanical models in my PhD, and came across many different types of models in the Commission, often used for policy impact assessments.\n\nWhat is a model?\nFirst of all, what is a model? Well, as usual there are many ways to divide and classify things, but one distinction is:\n\nPrinciple/process-driven models: these are models that are built based on understanding the physics or processes behind the system. A simple example is Hooke’s Law which describes how a spring extends under a given load. More complex examples are hydrological models and climate models. In all cases, the model is built based on some encoding of the physics or processes driving the system.\nData-driven models: in these models, the system is treated much like a black box. Instead, we use a set of measured system inputs and outputs, and try to build a statistical mapping between the two. A linear regression is a simple example, but more complex examples are Gaussian processes, neural networks, deep learning and so on. What they have in common is that the modelling simply tries to replicate the input/output relationship, rather than trying to model any particular physical process.\n\nNo doubt many people would dispute the nuances of those definitions, but I think that the core concept is solid. However, the two categories are not distinct, and in fact will overlap to some extent. At the end of the day, both categories are a system of equations, and both usually have to be calibrated or fitted in some way. This discussion could go on for a while so let’s leave it at that for now.\n\n\nElementary, my dear Occam\nOccam’s Razor is one of those heuristics that seems to apply everywhere. According to Wikipedia, it can be defined as:\n\nthe problem-solving principle that “entities should not be multiplied without necessity”, or more simply, the simplest explanation is usually the right one.\n\nWhat has this got to do with modelling? Well, everything points to the fact that the simplest model that explains the data/process is the best.\nThis idea is well-known in statistical modelling (i.e. category 2 of the taxonomy above). You might have heard of the “bias-variance trade off”: this is the idea that there is a balance to be struck between underfitting and overfitting a set of data. Underfitting means that the model is too simple to explain the data/process, whereas overfitting means the opposite: the model gives too much weight to individual observations, rather than focusing on the underlying process.\nThe general idea is that most of the time, when you are modelling data, you expect a relationship along the lines of:\n\\[ y = f(x) + \\epsilon \\] where \\(y\\) is the variable you want to model, \\(x\\) is the variable, or set of variables that explain \\(y\\), and \\(\\epsilon\\) (which can also be a function of \\(x\\)) is a summary of other “things” that contribute to \\(y\\) but you are not explicitly trying to model. The “things” can errors due to measurement, but they can also be other variables that you are not able to measure, or you prefer not to include in this particular modelling exercise.\nThe point is that what you want to understand and isolate is \\(f(x)\\), not \\(\\epsilon\\). If your model starts to include elements of \\(\\epsilon\\), then you have a mix of the two quantities, and this makes it difficult/impossible to (a) understand the relationship between \\(y\\) and \\(f(x)\\), and (b) make predictions of \\(y\\) at unobserved values of \\(x\\). Occam’s Razor can be statistically proven - a nice example is in David Mackay’s excellent Information Theory, Inference, and Learning Algorithms book - see Chapter 28.\nAnyway, let’s visualise this idea. Here’s some data: \\(x\\) is random numbers, \\(y = 2x + \\epsilon\\), where $ $ is normally-distributed noise.\n\nlibrary(plotly)\nx &lt;- runif(20) %&gt;% sort() # random numbers (sorted to avoid problems in plotting)\ny &lt;- 2*x + rnorm(20)*0.5 # create y and add noise\ndf &lt;- data.frame(x, y)\nfig &lt;- plot_ly(data = df, x = ~x, y = ~y) # plot\nfig\n\n\n\n\n\nNow let’s consider two possibilities. We want to know how \\(y\\) is related to \\(x\\). In the first case, we fit a straight line through the data (this is cheating because it is the real relationship).\n\nf &lt;- lm(y ~ x, data=df) # fit linear regression\ndf &lt;- cbind(df,ylin=f$fitted.values) # add to data frame\n\n# plot\nfig &lt;- plot_ly(data = df, x = ~x)\nfig &lt;- fig %&gt;% add_trace(y = ~y, name = 'Data',mode = 'markers')\nfig &lt;- fig %&gt;% add_trace(y = ~ylin, name = 'Linear fit',mode = 'lines')\nfig\n\n\n\n\n\nLooks good, and it is close to the actual relationship. But in practice, if all we have is a set of \\(x\\) and \\(y\\) values, we wouldn’t know whether the relationship between \\(x\\) and \\(y\\) is linear, or something more complicated. We might consider fitting a higher-order polynomial, for example. This might seem like a good idea, because after all, a linear model is a special case of a higher order polynomial, so we can’t lose anything by making it more complicated - it will still provide the best fit, right?\n\nf2 &lt;- lm(y ~ poly(x,5), data=df) # fit 5th-order polynomial\ndf &lt;- cbind(df, ypoly=f2$fitted.values, ytrue = 2*x) # add to data frame\n\n# plot\nfig &lt;- plot_ly(data = df, x = ~x)\nfig &lt;- fig %&gt;% add_trace(y = ~y, name = 'Data',mode = 'markers')\nfig &lt;- fig %&gt;% add_trace(y = ~ytrue, name = 'True f(x)',mode = 'lines')\nfig &lt;- fig %&gt;% add_trace(y = ~ylin, name = 'Linear fit',mode = 'lines')\nfig &lt;- fig %&gt;% add_trace(y = ~ypoly, name = '5th-order poly fit',mode = 'lines')\nfig\n\n\n\n\n\nAs you can see, this was not a good idea. The higher-order model overfits the data. Because it has more flexibility in its shape, it is able to get closer to some of the outlying points, but by doing so it is confusing the noise with the underlying linear relationship, \\(f(x)\\).\n\n\nKeep on Occam in the Real World\nOK, so outside of regression, what’s the implication? Well, as we have discussed here, any model, be it a large physical model or a simple regression, is a mapping of inputs to outputs. It aims to emulate a system. If your model is calibrated to observations (if it’s not, that’s a problem in itself), then you are in exactly the same situation as the example above. In short, if you build a hugely complicated model you run the risk of overfitting it to observation data, which actually results in poorer prediction capability than a simple model.\nIt’s important to point out amid all this complex-model-bashing that this is not about ignoring complexity, but striking a balance. There is as much risk in underfitting as overfitting. Clearly, you can’t model a complex nonlinear system with a linear model either. So, what to do?\nLuckily these problems have been studied by very clever people for many years, in a field known as model selection, and there are a range of tools such as information criterion, Bayes factors, cross-validation and more.\nA problem here is that while it is easy in statistical modelling to build many different alternative models and compare them, in process-driven models this is much more difficult. While alternative models can be compared, it is probably worth keeping in mind that adding more and more complexity to a model, and throwing more computing power at it, does not necessarily reduce the uncertainty. To make that point, consider how the uncertainty in climate sensitivity has changed over the last 30 years:\n\n1979: 1.5-4.5C [National Academy of Sciences]\n1990: 1.5-4.0C [IPCC first report]\n1996: 1.5-4.0C [IPCC second report]\n2007: 2.0-4.5C [IPCC fourth report]\n2014: 1.5-4.5C [IPCC fifth report]\n\nSince 1979, computing power has increased at least a hundredfold. But the estimated uncertainty has actually remained the same.\n\n\nFinally\n\n\n\nModel tradeoff\n\n\nThis is a slightly whimsical slide that I’ve used in the past to elaborate on this problem. A simple model is further away from reality in terms of “structural uncertainty”, i.e. the uncertainty due to the fact that the model simplifies the complexities of the real system. But there is actually less uncertainty in its parameters because there are less parameters. In the linear model, there are only two parameters.\nWhereas, if we go to a more complex model, it becomes closer to “reality” in that it inludes more of the complexities of the real system, but now we have more parameters to fit, so there can actually be more parametric uncertainty! In the fifth-order polynomial used above, there are 6 parameters, and it was clear that it actually caused more trouble than it was worth.\nOk that’s it for now folks. This is a long topic and I have other work to get on with. I intend to continue this series on the ins and outs of modelling and uncertainty.\nImage by Clker-Free-Vector-Images from Pixabay"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "William Becker",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n      Google Scholar\n  \n  \n      ResearchGate\n  \n\n  \n  \nI am a policy-oriented data scientist currently working at European Centre for Medium-Range Weather Forecasting in Bonn as a Training Specialist in Machine Learning.\nUp until May 2024 I was a freelance data scientist based in North Italy. I worked for international organisations like the European Commission, UN agencies, policy consultancies and non-profit organisations on topics from GHG emissions and sustainable development to skills systems and innovation.\nI’m eternally curious about learning new things, and this has led me to work in many different fields. I’ve worked on high-impact projects with dozens of international organisations.\nThroughout my career I’ve discovered what I really enjoy:\n\nWorking with interesting people\nTackling challenging and worthwhile problems\nCreating things: tools, books, reports, training courses\nThe satisfaction of doing something well\nLearning new things and teaching others\n\nTo find out more about my work, see my recent projects. For my research, see my publications list.\n\n CV downloads\nShort CV Europass CV Academic CV"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "William Becker",
    "section": "     Experience",
    "text": "Experience\n\n\n\n\n\n\nTraining specialist in machine learning\n\n\n\n2024-present\nDesigning and managing an international training programme for machine learning (ML) in weather and climate at the European Centre for Medium-Range Weather Forecasting. Involves mapping user needs, managing procurements and coordinating with stakeholders in European national meteorological and hyrological services.\n\n\n\n\n\n\n\n\nData science/policy consultant\n\n\n\n2020-2024\nStatistical, software and data research for international organisations including the European Commission, UN agencies, consultantcies and non-profits. I lead projects from client/stakeholder engagement and workshops through data analytics and web visualisation.\nSee more on the projects page.\n\n\n\n\n\n\n\n\nSenior data/policy analyst\n\n\n\n2014 - 2020: European Commission, Joint Research Centre\nSupporting the European Commission in international data analysis, indicator frameworks, and analysis of uncertainty and sensitivity/robustness. I was a senior researcher simultaneously in the European Commission’s Competence Centre for Composite Indicators and Scoreboards, as well as the Competence Centre for Modelling, which are both cross-cutting groups supporting multiple policy areas. I worked extensively with many different departments of the European Commission (JRC, EEAS, BUDG, GROW, SG, RSB, REGIO, etc), leading and supporting projects and training colleagues, as well as with a wide range of international organisations.\n\n\n\n\n\n\n\n\nScientific/technical project officer\n\n\n\n2011 - 2014: European Commission, Joint Research Centre\nI worked in the Econometrics and Applied Statistics unit of the JRC: my role was to provide statistical support to the rest of the European Commission, particularly regarding impact assessments, indicators and modelling."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "William Becker",
    "section": "   Education",
    "text": "Education\nDownload my CVs using the buttons above for more details.\n\nPhD Mechanical Engineering | University of Sheffield, UK, 2006 – 2011: PhD in Mechanical Engineering and applied machine learning and Bayesian stats. Thesis entitled: Uncertainty propagation through large nonlinear models.\nMEng Mechanical Engineering | University of Sheffield, UK, 2002 – 2006: MEng in Mechanical Engineering with Spanish. Included one year as an Erasmus student at the University of Seville, Spain."
  },
  {
    "objectID": "about.html#links",
    "href": "about.html#links",
    "title": "William Becker",
    "section": "   Links",
    "text": "Links\nHere are a few links with more info and projects related to me in a professional context.\n\nI work on indicator and data projects with an excellent team of professionals. Find out more at our website here, including my blog pages and details about my colleagues.\nSee my (recent) code at my GitHub page.\nMy older personal/professional website is still up here.\nLinkedIn\nResearchgate and Google Scholar"
  },
  {
    "objectID": "about.html#outside-work",
    "href": "about.html#outside-work",
    "title": "William Becker",
    "section": "    Outside work",
    "text": "Outside work\nI am a creative person and a musician and I enjoy playing guitar and (trying to) sing. I also love sports, including running, skiing, swimming and hiking. I spend a lot of time with my family, but also like building things and growing organic food in the garden.\nA couple of voluntary activities I am involved in:\n\nI manage children’s and adult’s music courses at the JRC Association for Music Makers.\nI founded and run a Repair Cafe in Ispra where we help people fix and reuse broken items rather than throwing them away."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "William Becker",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n      Google Scholar\n  \n  \n      ResearchGate\n  \n\n      \nI’ve led data-driven research projects for international organisations since 2011 on topics from sustainable development to globalisation.\nI currently work as a training specialist in machine learning at the ECMWF in Bonn. Formerly, I worked for many institutions including the European Commission, UN agencies, consultancies and non-profit organisations.\n\nFind out about my projects\nSee my academic publications\nRead what I’ve been up to lately\nMore about me, including CVs\n\n\n    \n    \n  \n\n\n\n\nI work with"
  },
  {
    "objectID": "posts/2022-09-08_Genesis/index.html",
    "href": "posts/2022-09-08_Genesis/index.html",
    "title": "Genesis",
    "section": "",
    "text": "I thought it was about time I made a personal website, and so here it is.\nThe reality is, in this day and age (yes I’m getting a little old), having a personal web page is very handy for showcasing my work to potential employers, for generating professional collaborations, and for keeping a record of what I’ve done. Instead of boring people with a long CV, I can just point people to this web page and they can read as much or as little as they want.\nI’m not sure yet how much blogging I will do here. I already have a series of blog posts on composite indicators at another professional page (with some colleagues). I also have a consultancy site here but under the guise of “bluefox”, and there are other blog posts there. I may end up porting some of those here.\nAt the end of the day, blogging is a lot like shouting into the infinite void of space. Most of the time no one is listening, but it can be fun anyway!"
  },
  {
    "objectID": "posts/2022-10-11_COINr_Paper/index.html",
    "href": "posts/2022-10-11_COINr_Paper/index.html",
    "title": "Paper for COINr published",
    "section": "",
    "text": "Yesterday I was very pleased to have my paper, entitled “COINr: An R package for developing composite indicators”, published in the Journal of Open Source Software (JOSS). You can find the paper in all its glory right here.\nAs has been said elsewhere, JOSS follows a really interesting open review and publication system via GitHub. I can safely say it has been the nicest publication experience I have had so far. And that’s not because it was accepted with no modifications. On the contrary, I had to expand the unit test coverage from about 25% up to 80% which took quite a lot of effort, and there were a number of other good issues raised by the reviewers. The reason I liked it is because it was fair and thorough but also quick and efficient. You can see the full review here.\nSo COINr now has a proper citation associated with it, which means that I and my co-authors can be credited for our work. It also has an all-important DOI.\nAt the moment, this is the last paper I am involved in. I have resolved not to get involved in other papers unless I return to an academic job. However, there may be some other interesting things on the horizon for COINr. There is some fairly serious talk of turning it into a Shiny app, which would be very interesting. I can’t reveal other details at the moment since nothing is totally confirmed but stay tuned!"
  },
  {
    "objectID": "posts/2022-12-16_Freelancing/index.html",
    "href": "posts/2022-12-16_Freelancing/index.html",
    "title": "Freelance data science: a two-year review",
    "section": "",
    "text": "I started working as a freelancer back in October 2020. Since then I’ve worked on all kinds of projects and it’s been a real journey: sometimes hectic, sometimes slow, but always unpredictable. After more than two years, I thought a long and rambling review was in order, so here it is!"
  },
  {
    "objectID": "posts/2022-12-16_Freelancing/index.html#a-new-era-sunrise",
    "href": "posts/2022-12-16_Freelancing/index.html#a-new-era-sunrise",
    "title": "Freelance data science: a two-year review",
    "section": "A New Era :sunrise:",
    "text": "A New Era :sunrise:\nUp until March 2020 I worked at the European Commission’s Joint Research Centre (JRC). I worked there for nine years, and it was a really fantastic experience supporting European policy making, doing research, meeting and working with great people, and living in a rather nice corner of Italy near the mountains and the lakes. Unfortunately, while the JRC is great for temporary contracts, permanent contracts are much harder to come by. And by early 2020 my time was up, and as a Brexit refugee I was also excluded from applying to the Commission at all. Thanks Boris. :clown:\nMarch 2020 also coincided with the start of the pandemic and a long lockdown in Italy, so my first months outside the JRC were not the honeymoon of casual family time, projects and sports that I had envisaged. Anyway, for various reasons it was clear that we (me and my family) would stay in the area for a while yet. And so after a few months I began looking for work options.\nThe problem is that living in a small town in a mostly rural area in North Italy, on-site work options in my line of work basically consist of:\n\nThe JRC\nSee 11\n\nAnd so, this was how I found myself looking at freelance work. One of the few positive outcomes of the pandemic was that remote work became much more normalised, and so this was more feasible than previously. And so I began my journey as a self-styled “freelance data scientist”."
  },
  {
    "objectID": "posts/2022-12-16_Freelancing/index.html#the-journey-tent",
    "href": "posts/2022-12-16_Freelancing/index.html#the-journey-tent",
    "title": "Freelance data science: a two-year review",
    "section": "The Journey :tent:",
    "text": "The Journey :tent:\nIt really has been a Journey. Coming out of the JRC I realised I had a mixed bag of skills and experience: technical skills in statistics, indicators, modelling and sensitivity analysis; but also a lot of experience in various areas of policy - in sustainable development, connectivity, budgeting, and countless other areas that I had worked in. I had also written a lot of academic papers, organised and participated in numerous scientific conferences, and somehow built up a fairly wide network of friends and colleagues that I had worked with over the years.\nSo, if I was to become a freelancer, what kind of work should I actually do?\nLuckily, work came my way and answered this question for me. I was first contracted by the JRC to build an R package for composite indicators, which turned into the rather successful COINr package. During this time I also began working with WIPO to build a similar custom R package for the Global Innovation Index, and with UNIDO to help build the Quality Infrastructure for Sustainable Development Index.\nI learned an enormous amount during these three projects. Whereas programming had been an occasional past-time in my time at the JRC, now it became my main work. I had to quickly improve my programming skills to a more professional level - learning about GitHub, version control, functional and object-oriented programming, unit testing, continuous integration and other concepts that I had little knowledge of before.\nAlong the way I also worked on some smaller projects, including the WEF Nexus Index. There I met the data visualisation experts at OneTandem, with whom I also began to work on other projects. Together with another colleague Hedvig Norlén, we launched the compositeindicators.com website where we offer services related to composite indicator construction.\nIn the end, 2021 got a bit too busy so I decided to take a break in early 2022, and I had a relatively quiet period up until early Summer when things started heating up again. And now, at the end of 2022 I have loads of work in the pipeline and I’m busy. So busy in fact, that I am procrastinating by writing a long blog post about the last two years!\nBut apart from all the professional skills that I have learned, I also really learned a lot about freelancing in general. And that’s why I thought I’d talk about some pros and cons. So here they are."
  },
  {
    "objectID": "posts/2022-12-16_Freelancing/index.html#the-good",
    "href": "posts/2022-12-16_Freelancing/index.html#the-good",
    "title": "Freelance data science: a two-year review",
    "section": "The Good",
    "text": "The Good\nI get to work on all kinds of topics :rainbow:\nProbably the best thing about my work is the variety. I have been lucky to work on so many topics, and with so many interesting people, over the last two years. Here are some topics that I’ve worked as a freelancer, or am about to begin working on:\n\nInnovation\nQuality infrastruture\nSustainable development\nBioeconomy\nMedical diagnosis\nWater/Energy/Food\nInternational connectivity\nMigration\nAir pollution\nEmployee skills\nImpact assessment\nSensitivity analysis\n\nThat’s quite a good list! Every time I start on a new project I’m fascinated to learn about the new topic. Ok, some topics are more interesting than others, but to me it’s a great privilege to be able to work in all these different fields. It’s the great thing about stats and data science, it can really be applied to almost anything! And my “clients” are spread over five continents at the last count.\nI am Il Capo of my time :hourglass:\nWorking freelance is extremely flexible. I can work to my own timetable, I can take days off without asking anyone. I can get up and work in the middle of the night if I want to (I don’t, but maybe that option will come in handy one day…?). This does also come with downsides (see later) but the good part is that, as long as I get the job done for my clients, and work the time I am paid to work, I can completely manage my own time. For example, in quieter periods I have sometimes just worked mornings and spent the afternoons doing other things like sport, house and family tasks, music etc. I can also work from anywhere, in any country.\nI am El Jefe of my work :tophat:\nAnother great thing is that I get to decide which work to do, and I can use whatever tools I like to get the job done (as long as my clients agree). Working in the European Commission, you are restricted to using a lot of corporate tools and you may have to ask to install new software, and so on. Now, I do all my work on open-source software, I have everything saved on the cloud so I can access all of my work, even on my phone. Also, if I don’t like a project, I can turn it down.\nThere’s less admin :page_facing_up:\nOk this is not completely true (see later) but some admin-related tasks disappear. I don’t have to produce briefings and I don’t have to attend many meetings. Of the meetings I do attend, they are usually short and well-focused. I don’t have to have performance review conversations, and I manage all my own IT, billing, leave and other things. I recall many difficult interactions with the more bureaucratic arms of the EC, although it’s just a fact of working in a very big organisation. Still, I don’t miss that part at all!"
  },
  {
    "objectID": "posts/2022-12-16_Freelancing/index.html#the-bad",
    "href": "posts/2022-12-16_Freelancing/index.html#the-bad",
    "title": "Freelance data science: a two-year review",
    "section": "The Bad",
    "text": "The Bad\nOk, now let’s see the other side of the coin!\nI’m a Lone Wolf :wolf:\nAlthough I work for many people, and I have even worked with some great colleagues, there’s no denying that freelancing is a bit of a lonely business. I work completely remotely, which is also a good thing, but it means that most days I work on my own. Of course, I have my family around, and I have meetings every now and then with clients, but I’m a social animal (half of the time) and I do miss being part of a team, working together with colleagues and friends in person and having coffee breaks and lunches together.\nWhen you work in an organisation and in a team there is also a feeling of being “part of something”, that you miss as a freelancer. I’m a mercenary, a hired gun - I do the work I’m paid to do and then when it’s done, I move onto something else. This is also a good thing, but I guess like the classic wandering cowboys, you do miss the feel of having a home! Oh yes this is getting poetic!\nWorkload goes up and down :roller_coaster:\nLike any job, freelancing workload goes through quiet and busy periods. The difference is that if you work for an employer (i.e. a “normal job”) you are paid the same regardless. You have a fixed salary per month, and it is the employer’s responsibility to ensure that your workload is not too much or too little.\nAs a freelancer, if you have less work, you have less money. So if you go through a quiet period you have to spend time looking for work. At the other end of the spectrum, it is easy to overload yourself with too many contracts and end up having to work during the evenings as well. It’s very difficult to get exactly the right amount of work, with contracts overlapping, starting and stopping, and meeting deadlines on various projects. This is something that gets easier with experience but is still a challenge.\nNo work, no pay :gun:\nIf I’m not working on a paid project then I’m not earning money. This is obvious but comes with some implications. First, if you are sick, you don’t get paid. Luckily I’m not often sick, but if I were to be sick for a long period of time, I would have no income. There’s no safety net.\nSecond is that you can’t easily do unpaid but interesting activities that you might have done as an employee. For example, when I was working at the JRC I could spend some time on academic research, and I went to many conferences. I also frequently gave talks at seminars, lectures at universities and so on. This was possible because I could do it during my working time, and the JRC would pay for me to travel to conferences (within reason). As a freelancer, I have also been invited to give talks at workshops and to collaborate on papers but usually I have to turn these down. If I spend the morning giving a talk, or working on a paper, I have to give up half a day’s pay. This is OK sometimes but hard to justify in many cases. And so, I end up focusing mostly on paid work.\nAnd finally of course, if I don’t find contracts, I don’t get paid. This means that there is very little job security. True, so far I have not had trouble finding work, but who knows for the future?\nThere’s more admin :page_facing_up:\nDid I say there was less admin? Well, actually I’m not sure. There are a lot of admin things to do as a freelancer. You have to draw up contracts and proposals for contracts, bill clients, talk to accountants, deduct expenses, and carefully monitor your time on each project, tracking the progress of deliverables and making sure you complete the work in the required time. There is also a lot of deliberation about how to define deliverables, whether to charge by deliverables or by time, estimating costs and so on. Actually it all adds up rather quickly.\nToo much flexibilty? :worried:\nFlexibility is great, but the price is that work and home life can blur so much that you find yourself in a permanent limbo between an “on” and “off” state. This is compounded by the fact that I work at home. When I began freelancing, I often worked in the evenings to “get a bit extra done”, knowing that whenever I work, my time is paid. However, I learned after a while that you have to try to draw a line between your working day and your non-work time. Now, I work fairly rigidly 8am-5pm, but I still occasionally find myself working a little in the evenings, or else drifting to non-work tasks during the day. I guess it just requires a measure of time management and discipline."
  },
  {
    "objectID": "posts/2022-12-16_Freelancing/index.html#the-summary",
    "href": "posts/2022-12-16_Freelancing/index.html#the-summary",
    "title": "Freelance data science: a two-year review",
    "section": "The Summary",
    "text": "The Summary\nFreelancing has been great for me and a huge learning experience. I think the main good things are the variety, the flexibility, and the sense of being in charge of your own destiny. On the other hand, it is quite a solitary experience, you have to be quite disciplined with your time and it comes with little job security.\nApart from learning all kinds of professional skills, I also learned some overall lessons. First is that contacts are all-important. I’m not a networker in the sense of someone who sidles up to “important” people at conferences or cold-calls potential clients. But I am someone who is very open to working with anyone on any kind of project, as long as it is interesting. This attitude, I think, has paid off: when I left the JRC I had worked with many people in many places and this led to recommendations and work. The fact is that if you get the work done to a good standard, and you are an easy person to work with, over time this gets noticed and you don’t have to aggressively network to build up a good list of contacts.\nAnother thing I learned is that academic papers are not very important outside of academia. Probably not a surprise to anyone, but unless you are planning on working in academia, no one really cares about how many citations you have!\nI also learned that having a broad profile has its pros and cons. Whereas I tend to think of myself as a “jack of all trades”, quite frequently I think I come across (to people who don’t know me) as a rather a “master of none”. I think that’s due to the fact that I have always loved learning new things and working in new fields, but try putting this on a CV when applying for a job. Usually employers like for people to fit into neat categories, and if your CV is very mixed, its hard to make head or tail of it. I still think a broad profile is a strength, but communicating it as a strength is really a challenge.\nI’ll end this extremely long post with a last positive thought about freelancing. Working at the JRC, like any big organisation, can feel a bit like being in a bubble. This is well-known in the JRC, and its the result of bring a lot of sciency people together from different corners of Europe on temporary contracts - naturally they tend to socialise together and talk about JRC stuff, and it can be easy to not get so involved in the outside world. At least for the years that you are working there, the JRC offers a great level of job security and benefits. In short, the bubble is quite cozy.\nWhen I left the JRC, therefore, it was also a test of whether I could stand on my own feet in the cold world outside of the bubble. And I’m glad to say after two years that I can. I have managed to get regular and interesting work, and I could probably carry on doing this almost indefinitely. So, although I may at some point seek the job security and the company of friends and colleagues back in Commission or another big organisation, I do at least know that, if the bubble bursts, I’ll be absolutely fine!"
  },
  {
    "objectID": "posts/2022-12-16_Freelancing/index.html#footnotes",
    "href": "posts/2022-12-16_Freelancing/index.html#footnotes",
    "title": "Freelance data science: a two-year review",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be fair, you could also travel to Milan every day, but… nah.↩︎"
  },
  {
    "objectID": "posts/2023-07-28_A2SIT_launch/index.html",
    "href": "posts/2023-07-28_A2SIT_launch/index.html",
    "title": "A2SIT App launched for UNHCR",
    "section": "",
    "text": "’ve just come to the end of a very interesting contract working for the UNHCR in Guatemala (for the moment).\nAs you might know, the UNHCR helps displaced people around the world and protects human rights. In many places in the world there are movements of forcibly diplaced people, including refugees. Guatemala is one place where people are usually moving from South to North seeking refuge, asylum or simply a better life.\nOne issue faced by UNHCR staff is understanding where in the country to direct their limited resources to help people. There are many factors that can be used to decide where to intervene, and the picture can rapidly become complex.\nTo help this situation, I was contracted by UNHCR to build an app which can be used to construct composite indicators of “severity” at the municipal level (called “Admin-2” level by the UNHCR classification) in Guatemala.\nThe app, which is called the Admin-2 Severity Index Tool (A2SIT) is a fairly complex Shiny app which runs on an R server. It is encapsulated in an R package that I also built called A2SIT which is available on GitHub. I should mention that although I built the app and package, a huge amount of work was done in collecting the data, helping guide the app and designing the index, as well as many other things during the project.\nIn the app you can:\n\nUpload your own data set which should be at the Admin 2 level for one of the supported countries\nObtain a statistical analysis of your data and flagged issues for indicators\nCalculate index results and plot them on a map, and as tables and bar charts\nPlay with composite indicator settings\nView and compare profiles of municipalities\nCompare scenarios\n\nThere is also fairly comprehensive documentation which is available in an online book!\nThis has been a big project and I have learned a lot. But it has also been very rewarding. The app has been successful in testing with users, and should be rolled out to other countries in Central and South America. There is also a Medium article on the app.\n\n\n\nData analysis\n\n\n\n\n\nMapping and results\n\n\n\n\n\nRegion profiles"
  },
  {
    "objectID": "posts/2023-09-28_COIN_training/index.html",
    "href": "posts/2023-09-28_COIN_training/index.html",
    "title": "COIN training on composite indicators",
    "section": "",
    "text": "So on the 28th of September I gave a couple of invited talks at the 2023 JRC Week on Composite Indicators and Scoreboards, whic is spread over a few days and delves into all the technical and non-technical intricacies of building and analysing composite indicators.\nI have helped with this training course for some years, particularly when I worked full time in the JRC’s Competence Centre for Composite Indicators and Scoreboards. In this case I gave two lectures:\n\nQuality control and robustness, in which I explain the main concepts for testing robustness (sensitivity analysis) and give some tips for building high-quality composite indicators.\nA lecture on building and auditing composite indicators with R\n\nFor the first lecture you can find the slides here, and the video you can find here (the lecture starts about 14:35).\nIn the second lecture I built a code demo which is available as an HTML page on GitHub here. In this notebook, I show how to:\n\nDownload data from the Eurostat API in R\nPass the data and structure into COINr\nAnalyse the data and calculate the results\nMap the data interactively using Leaflet\nExplore some methodological variations\nExport everything to Excel\n\nThis encapsulates many of the points that I think go in to making a good composite indicator. If you want to hear more about it, check out the recording and skip to approximately 16:28. Unfortunately there were a few technical problems throughout the lecture so you may need to skip ahead on occasions!"
  },
  {
    "objectID": "posts/2023-11-10_Vienna_workshop/index.html",
    "href": "posts/2023-11-10_Vienna_workshop/index.html",
    "title": "Expert group meeting on quality infrastructure in Vienna",
    "section": "",
    "text": "Hot on the heels of the Riyadh workshop on Quality Infrastructure (QI), I attended the Expert Group Meeting (EGM) for the QI4SD Index, hosted at UNIDO headquarters in Vienna, Austria.\nThe objective of the workshop was to present the methodology of the QI4SD Index in detail to a group of around 40 experts from QI organisations including ISO, IEC, BIPM, OIML and many representatives from national standards bodies and other QI-related institutions, in order to receive feedback to improve the index in its next update.\nThe QI4SD Index is an index for measuring quality infrastructure and its contributions to SDGs, which I helped to build starting in 2020. It was launched in 2022, and has since gained quite some visibility and has been adopted by some countries as a national KPI for measuring progress. Subsequently, UNIDO would like to launch and updated version of the index in 2024. The workshop therefore feeds directly into improving the index.\nThe outcomes of the workshop were many pages of excellent feedback which we are now digesting. As usual in these workshops, some suggestions are more feasible than others, and we can only do so much in the update next year, but all feedback was extremely welcome.\nWe should begin working on the index update early in 2024."
  },
  {
    "objectID": "posts/2023-11-20_WEF_update/index.html",
    "href": "posts/2023-11-20_WEF_update/index.html",
    "title": "Updates of the WEF Nexus Index",
    "section": "",
    "text": "The WEF Nexus Index is a composite indicator which measures integrated resource management and sustainable development across the three sectors of Water, Energy and Food.\nI have been involved in this index for several years and it has been an interesting project getting a glimpse into the the world of resource management. Recently I was responsible for calculation of the latest results, so that the next version can be launched soon.\nSneak peek of who is top of the rankings….? In first place, still, is Iceland! Not really surprising given it has a lot of resources in terms of water, geothermal energy and so on."
  },
  {
    "objectID": "posts/2023-12-21_Wrapup2023/index.html",
    "href": "posts/2023-12-21_Wrapup2023/index.html",
    "title": "2023 Wrap Up",
    "section": "",
    "text": "So it’s the end of 2023, I’ve closed all the big projects that needed closing before Christmas, and thought that a little summary of the year was in order.\n2023 was a very busy but also very interesting year for me. As usual in freelance mode, things go up and down quite a lot: the start of the year was fairly manageable, June and July were crazy-busy, the summer was a nice break but I paid for that dearly in September until the end of November period in which a lot of big projects needed finishing at the same time and I nearly melted! Now at the end of December I’m feeling pretty relaxed and everything is fine. Which is why I finally have some time to write something here.\nSo, looking back on the year, here is how it went."
  },
  {
    "objectID": "posts/2023-12-21_Wrapup2023/index.html#ghg-emissions-analysis",
    "href": "posts/2023-12-21_Wrapup2023/index.html#ghg-emissions-analysis",
    "title": "2023 Wrap Up",
    "section": "GHG Emissions Analysis",
    "text": "GHG Emissions Analysis\nThings kicked off in early January when I started working half-time as a data scientist at the European Commission’s Joint Research Centre (JRC), in the EDGAR group which produces and manages a global database on greenhouse gas (GHG) emissions. Although I’d previously worked in the JRC for quite a few years, this was a completely new topic and it took me a while to find my feet and learn all about different GHG emissions, how they are estimated, and the technical ins and outs of the database. After this burn-in period however, and some sharpening my SQL skills, I managed to produce a custom R package and app which allows uncertainty estimation of GHG emissions across any sector, country, substance and time point. Here’s a little snapshot of the uncertainty estimations:\n\n\n\nTotal GHG emissions for top emitters\n\n\nIt looks quite straightforward but a lot of work went into it!\nAnyway my JRC EDGAR work has been the stable half of my work for the whole of 2023 and I’m glad to say I will be continuing with the group well into 2024."
  },
  {
    "objectID": "posts/2023-12-21_Wrapup2023/index.html#community-based-protection-tool-for-unhcr",
    "href": "posts/2023-12-21_Wrapup2023/index.html#community-based-protection-tool-for-unhcr",
    "title": "2023 Wrap Up",
    "section": "Community-based protection tool for UNHCR",
    "text": "Community-based protection tool for UNHCR\nAnother big project that has run through 2023 is building the A2SIT app for UNHCR Guatemala. This started in January when I was contracted to begin work building the back-end code to build a composite indicator to understand which municipalities were most in need of community-based protection in Guatemala. The UNHCR team were in the process of collecting indicators and data, and I helped them build the composite indicator and the data pipeline to calculate index scores, using as usual the COINr package. At the end of the initial contract, I also floated the idea that I could build the front end of the app using Shiny, and this led to more work with them, and now I’m very proud to say the A2SIT app is deployed and has been presented to UNHCR offices in various countries as a general tool for measuring “severity”. I’m just wrapping up what will probably be my last contributions to the project as it is pretty much finished, but to give some highlights:\n\nThe app is built in Shiny using Shinydashboardplus\nIt takes a custom set of indicators and data uploaded in an Excel file\nIt builds a composite indicator of severity, returning results and statistics\nResults are plotted on interactive maps\nProfiles are generated for all regions\nUsers can upload their own shape files to build composite indicators for any set of regions or countries\nThe app is fully documented in an online book\n\n\n\n\nScreenshot of A2SIt app\n\n\nRead some more about the A2SIT app and project in a Medium article here."
  },
  {
    "objectID": "posts/2023-12-21_Wrapup2023/index.html#a-composite-indicator-app",
    "href": "posts/2023-12-21_Wrapup2023/index.html#a-composite-indicator-app",
    "title": "2023 Wrap Up",
    "section": "A Composite Indicator App",
    "text": "A Composite Indicator App\nEarly in 2023 I began working with the Foundation for Innovative New Diagnostics (FIND) to build an app which can build composite indicators for any set of indicators and countries/regions/units. After much hard work through 2023 with a number of talented people, we have deployed a Shiny app which is being trialled within FIND to help prioritise resources to promote medical diagnosis in developing countries. The app should soon also be released as an open-source R package.\n\n\n\nScreenshot of “Composer” app\n\n\nThe app (working name “Composer”) is effectively an accessible front end for the COINr package, including options for imputation of missing data, outlier treatment, normalisation, aggregation and global sensitivity analysis, not to mention various visualisations in maps, bar and bubble charts. Hopefully some more news on the open source version soon!"
  },
  {
    "objectID": "posts/2023-12-21_Wrapup2023/index.html#quality-infrastructure-for-sustainable-development",
    "href": "posts/2023-12-21_Wrapup2023/index.html#quality-infrastructure-for-sustainable-development",
    "title": "2023 Wrap Up",
    "section": "Quality Infrastructure for Sustainable Development",
    "text": "Quality Infrastructure for Sustainable Development\nContinuing my work with the United Nations Industrial Development Organisation (UNIDO), I began working on the update to the Quality Infrastructure for Sustainable Development (QI4SD) Index which I helped to create in 2020. The index, which has attracted some interest around the world and has been adopted by a number of countries as a national performance indicator, should have a full update in 2024.\nTo bring in as much expert feedback as possible for the update of the index, I worked with UNIDO colleagues to organise two expert workshops: one in Riyadh, Saudia Arabia; and the other in Vienna; in which we presented in detail the index methodology and discussed ways in which it can be improved.\nThe QI4SD Index should be released again in late 2024."
  },
  {
    "objectID": "posts/2023-12-21_Wrapup2023/index.html#global-innovation-index",
    "href": "posts/2023-12-21_Wrapup2023/index.html#global-innovation-index",
    "title": "2023 Wrap Up",
    "section": "Global Innovation Index",
    "text": "Global Innovation Index\nIn 2023 I continued an excellent collaboration with colleagues and friends in the World Intellectual Property Organisation (WIPO) working on the Global Innovation Index (GII). As part of an innovative team, I have helped redesign the back end of the GII, wrapping the modelling in a custom R package and contributing to documentation, data collection and auditing.\nI’m always pround to be part of the GII, which remains one of the highest-quality composite indicators that I have been involved with (and I have worked on many!), from the data collection, processing and auditing, up to the exploration and messaging of the results."
  },
  {
    "objectID": "posts/2023-12-21_Wrapup2023/index.html#european-skills-metrics",
    "href": "posts/2023-12-21_Wrapup2023/index.html#european-skills-metrics",
    "title": "2023 Wrap Up",
    "section": "European Skills Metrics",
    "text": "European Skills Metrics\nI was proud to be part of a winning consortium in a bid to update and maintain the European Skills Index (ESI) for Cedefop, a European agency which supports development of European vocational education and training policies and contributes to their implementation.\nWorking with the Fondazione Brodolini, an Italian consultancy, we have moved the calculation of the ESI to a fully-reproducible programmatic back end based around the COINr package. All data has now been collected and we expect to release the updated results in early 2024."
  },
  {
    "objectID": "posts/2023-12-21_Wrapup2023/index.html#other-things",
    "href": "posts/2023-12-21_Wrapup2023/index.html#other-things",
    "title": "2023 Wrap Up",
    "section": "Other things",
    "text": "Other things\nEven more things I worked on this year!\n\nI was commissioned to write a study by the European Commission on the prevalence of sensitivity analysis in impact assessments\nI ran the back-end modelling for the Water-Energy-Food Nexus Index, as I have done annually since 2019\nI audited the Vietnam Provincial Innovation Index, and was very pleased to meet the talented staff behind its creation near my home in Ispra, Italy!\nI made many further updates to the COINr package to which I have now made over 900 commits\n\nFinally I’m pleased to say that I have become an associate consultant at Technopolis Group, a consultancy that works extensively in European policy in Brussels (and also with many offices worldwide). Very happy to be on board and to be working on projects in the new future!"
  },
  {
    "objectID": "posts/2023-12-21_Wrapup2023/index.html#and-so",
    "href": "posts/2023-12-21_Wrapup2023/index.html#and-so",
    "title": "2023 Wrap Up",
    "section": "And so…",
    "text": "And so…\nAnd so. This has been a really busy but very rewarding year. Although I have been through some very busy periods I’ve had my time off and overall I think it has balanced out quite nicely.\nI’m extremely happy to have worked on so many interesting projects, with so many inspiring people. Here’s to another good year in 2024!"
  },
  {
    "objectID": "projects/A2SIT/index.html",
    "href": "projects/A2SIT/index.html",
    "title": "A2SIT app for community-based protection",
    "section": "",
    "text": "The United Nations High Commissioner for Refugees (UNHCR) is a UN agency that helps to protect the rights and wellbeing of people who have been forced to flee from their homes. In Guatemala, there are many such refugees and forcibly displaced people, typically moving from southern countries (such as Honduras, El Salvador) northwards. One of the activities of the UNHCR in Guatemala is to provide community-based protection (CBP) for these people, however due to the complexity of the situation more information is need to understand where CBP should be directed.\nMy work for the UNHCR is to build a web app to measure the vulnerability of populations at the municipal level in Guatemala by bringing together indicators measuring threats, socioeconomic circumstances, and response capacities to give a summary measure which helps to identify the municipalities that are in most need of CBP.\nThe resulting app, which is called the Admin-2 Severity Index Tool (A2SIT) is a fairly complex Shiny app which runs on an R server. It is encapsulated in an R package that I also built called A2SIT which is available on GitHub. I should mention that although I built the app and package, a huge amount of work was done in collecting the data, helping guide the app and designing the index, as well as many other things during the project.\nIn the app you can:\n\nUpload your own data set which should be at the Admin 2 level for one of the supported countries\nObtain a statistical analysis of your data and flagged issues for indicators\nCalculate index results and plot them on a map, and as tables and bar charts\nPlay with composite indicator settings\nView and compare profiles of municipalities\nCompare scenarios\n\nThere is also fairly comprehensive documentation which is available in an online book!\nThis has been a big project and I have learned a lot. But it has also been very rewarding. The app has been successful in testing with users, and should be rolled out to other countries in Central and South America. There is also a Medium article on the app.\n\n\n\nData analysis\n\n\n\n\n\nMapping and results\n\n\n\n\n\nRegion profiles"
  },
  {
    "objectID": "projects/Bioeconomy/index.html",
    "href": "projects/Bioeconomy/index.html",
    "title": "EU Bioeconomy Indicators",
    "section": "",
    "text": "I was contracted by the European Commission’s Joint Research Centre to provide a time series analysis of the EU Bioeconomy indicators data set. The analysis covered data from 1990-2022. Custom code was written and added to my COINr package, and a full analysis provided.\nI created a data pipeline which automatically analyses data after updates, and provides results tables to generate indicator dashboards. More info about the bioeconomy monitoring system here."
  },
  {
    "objectID": "projects/COINrApp/index.html",
    "href": "projects/COINrApp/index.html",
    "title": "Composer App",
    "section": "",
    "text": "The open-source COINr package is now used worldwide for building and analysing composite indicators. However, for those people who don’t use R, it is hard to access.\nThe Foundation for Innovative New Diagnostics (FIND) is a non-profit organisation which “seeks to ensure equitable access to reliable (medical) diagnosis around the world. Early in 2023 I began working with them to develop a Shiny app which can build composite indicators to help them identify in which countries to direct their resources.\nThe app is essentially a front end for the COINr package, but also has a number of modifications and tries to strike a balance between flexibility and not overwhelming the user with too many options. Its features include:\n\nUpload of any data set and index structure\nScreening units by data availability\nImputation of missing data\nOutlier treatment\nNormalisation and aggregation\nStats, maps, bar and bubble charts\nUnit profiles\nGlobal sensitivity analysis and reweighting\n\nI leave a couple of screenshots here, and hopefully the app will be made freely available in the near future!"
  },
  {
    "objectID": "projects/EDGAR/index.html",
    "href": "projects/EDGAR/index.html",
    "title": "EDGAR: Global emissions database",
    "section": "",
    "text": "EDGAR is “a multipurpose, independent, global database of anthropogenic emissions of greenhouse gases and air pollution on Earth”.\nAlthough most countries provide estimates of their greenhouse gas emissions, the timeliness and accuracy of these estimates may not always be sufficient. EDGAR provides completely independent and highly detailed estimates of emissions using activity data, which is particularly crucial in countries who may not have the capacity to accurately report emissions.\nMy work at the European Commission’s Joint Research Centre is to help with the EDGAR database, in particular to implement a tool for calculating uncertainty on emissions estimates following IPCC guidelines that interacts automatically with the database. I will also be involved in data collection and F-gases.\nThis is a new project started in 2023 so I am still getting familiar with the database, but so far it has been fascinating to learn how emissions are calculated and I am looking forward to getting deeper into the project."
  },
  {
    "objectID": "projects/ESI/index.html",
    "href": "projects/ESI/index.html",
    "title": "European Skills Index",
    "section": "",
    "text": "The European Skills Index (ESI) is a well-established composite indicator that is produced by Cedefop, the European agency which promotes vocational and education training. It aims to measure skills systems at the national level in European countries.\nAs of 2023 I was part of a consortium that successfully won the bid to update and potentially tweak/improve the ESI for the next few years.\nThe next iteration of the ESI will be launched in 2024, and I am in charge of the main ESI index calculations. Naturally, I am using the COINr package which is (even though I am mega-biased) the best tool out there for building reproducible composite indicators! Some particular improvements that I am making are:\n\nFully-reproducible data pipeline based on APIs as much as possible\nProgrammatic data and methodology validation\nResults calculated using COINr in Quarto documents: fully transparent and easily portable.\n\nThe idea here is to modernise the ESI calculation and make it super easy to come back in future years for updates.\nAt the time of writing we are about to calculate the final results. Of course there will be plenty to do afterwards in reporting, but for now we are on track."
  },
  {
    "objectID": "projects/IA/index.html",
    "href": "projects/IA/index.html",
    "title": "Impact assessment",
    "section": "",
    "text": "Impact assessment is the ex-ante assessment of the expected impacts of a policy. Usually this is done to compare several policy options, and the option which best meets the criteria will be chosen.\nOver several years I supported European impact assessments in several ways:\n\nAuthoring the European Commission’s Better Regulation Guidelines, which guide all European impact assessments, on the topics of modelling and sensitivity/uncertainty analysis.\nReviewing the statistical/data aspects of specific impact assessments, on topics including gender equality, GPS, long-term care and liability of nuclear energy companies.\nProviding training to Commission staff on my topics of expertise, especially on dealing with uncertainty in modelling."
  },
  {
    "objectID": "projects/other/index.html",
    "href": "projects/other/index.html",
    "title": "Other projects",
    "section": "",
    "text": "I have participated in many projects that are too many to list individually. A few can be mentioned here, in list format:\n\nMeasuring culture and creativity in European cities\nTracking adherence to chemical regulations\nMeasuring progress towards European sustainable development objectives\nAdvising on metrics for African policy making for the welfare of children\nTechnical input for metrics on financial secrecy and tax havens\nAuditing an index on global talent competitiveness\nTechnical input on sustainable development metrics\nAnalysis of the macroeconomic imbalance procedure\n\nSee also my academic research, and other technical reports from my days at the European Commission. My Researchgate profile has also further published works."
  },
  {
    "objectID": "projects/Training/index.html",
    "href": "projects/Training/index.html",
    "title": "Training",
    "section": "",
    "text": "I have been an enthusiastic teacher and trainer for many years. This includes:\n\nLeading and organising stats/data training courses for the European Commission\nTraining researchers and colleagues on composite indicators\nTraining researchers and colleagues on sensitivity and uncertainty analysis (see here and earlier editions)\nPrivate tuition on statistical programming, data analysis and indicators"
  },
  {
    "objectID": "projects/Vietnam/index.html",
    "href": "projects/Vietnam/index.html",
    "title": "Vietnam provincial innovation index",
    "section": "",
    "text": "The Vietnam Provincial Innovation Index is a new composite indicator developed by the Vietnam Ministry of Science and Technology in 2022, which aims to measure innovation at the provincial level in Vietnam. The index has been created in collaboration with the World Intellectual Property Organisation, who are the developers of the very well-known Global Innovation Index, which I have also helped out with on several occasions!\nIn November/December 2022 I was contracted to perform an independent audit of the index, including data and weighting checks, correlation and multivariate analysis, a review of the methodology and sensitivity and uncertainty analysis. I used the COINr package for the job, and since the developers were anyway using the package themselves to build the index, it was fairly easy to dig deep into the data and help out with one or two technical problems."
  },
  {
    "objectID": "publications/2018_SAScreening/index.html",
    "href": "publications/2018_SAScreening/index.html",
    "title": "Sensitivity analysis approaches to high-dimensional screening problems at low sample size",
    "section": "",
    "text": "About\nSensitivity analysis is an essential tool in the development of robust models for engineering, physical sciences, economics and policy-making, but typically requires running the model a large number of times in order to estimate sensitivity measures. While statistical emulators allow sensitivity analysis even on complex models, they only perform well with a moderately low number of model inputs: in higher dimensional problems they tend to require a restrictively high number of model runs unless the model is relatively linear. Therefore, an open question is how to tackle sensitivity problems in higher dimensionalities, at very low sample sizes. This article examines the relative performance of four sampling-based measures which can be used in such high-dimensional nonlinear problems. The measures tested are the Sobol’ total sensitivity indices, the absolute mean of elementary effects, a derivative-based global sensitivity measure, and a modified derivative-based measure. Performance is assessed in a ‘screening’ context, by assessing the ability of each measure to identify influential and non-influential inputs on a wide variety of test functions at different dimensionalities. The results show that the best-performing measure in the screening context is dependent on the model or function, but derivative-based measures have a significant potential at low sample sizes that is currently not widely recognised."
  },
  {
    "objectID": "publications/2019_SAgrouping/index.html",
    "href": "publications/2019_SAgrouping/index.html",
    "title": "Global sensitivity analysis for high-dimensional problems: How to objectively group factors and measure robustness and convergence while reducing computational cost",
    "section": "",
    "text": "About\nDynamical earth and environmental systems models are typically computationally intensive and highly parameterized with many uncertain parameters. Together, these characteristics severely limit the applicability of Global Sensitivity Analysis (GSA) to high-dimensional models because very large numbers of model runs are typically required to achieve convergence and provide a robust assessment. Paradoxically, only 30 percent of GSA applications in the environmental modelling literature have investigated models with more than 20 parameters, suggesting that GSA is under-utilized on problems for which it should prove most useful. We develop a novel grouping strategy, based on bootstrap-based clustering, that enables efficient application of GSA to high-dimensional models. We also provide a new measure of robustness that assesses GSA stability and convergence. For two models, having 50 and 111 parameters, we show that grouping-enabled GSA provides results that are highly robust to sampling variability, while converging with a much smaller number of model runs."
  },
  {
    "objectID": "publications/2020_bioecon/index.html",
    "href": "publications/2020_bioecon/index.html",
    "title": "Development of a bioeconomy monitoring framework for the European Union: An integrative and collaborative approach",
    "section": "",
    "text": "About\nThe EU Bioeconomy Strategy, updated in 2018, in its Action Plan pledges an EU-wide, internationally coherent monitoring system to track economic, environmental and social progress towards a sustainable bioeconomy. This paper presents the approach taken by the European Commission’s (EC) Joint Research Centre (JRC) to develop such a system. To accomplish this, we capitalise on (1) the experiences of existing indicator frameworks; (2) stakeholder knowledge and expectations; and (3) national experiences and expertise. This approach is taken to ensure coherence with other bioeconomy-related European monitoring frameworks, the usefulness for decision-making and consistency with national and international initiatives to monitor the bioeconomy. We develop a conceptual framework, based on the definition of a sustainable bioeconomy as stated in the Strategy, for a holistic analysis of the trends in the bioeconomy sectors, following the three pillars of sustainability (economy, society and environment). From this conceptual framework, we derive an implementation framework that aims to highlight the synergies and trade-offs across the five objectives of the Bioeconomy Strategy in a coherent way. The EU Bioeconomy Monitoring System will be publicly available on the web platform of the EC Knowledge Centre for Bioeconomy.\n\nDashboards for the monitoring system are now available here."
  },
  {
    "objectID": "publications/2020_Metafunc/index.html",
    "href": "publications/2020_Metafunc/index.html",
    "title": "Metafunctions for benchmarking in sensitivity analysis",
    "section": "",
    "text": "About\nComparison studies of global sensitivity analysis (GSA) approaches are limited in that they are performed on a single model or a small set of test functions, with a limited set of sample sizes and dimensionalities. This work introduces a flexible ‘metafunction’ framework to benchmarking which randomly generates test problems of varying dimensionality and functional form using random combinations of plausible basis functions, and a range of sample sizes. The metafunction is tuned to mimic the characteristics of real models, in terms of the type of model response and the proportion of active model inputs. To demonstrate the framework, a comprehensive comparison of ten GSA approaches is performed in the screening setting, considering functions with up to 100 dimensions and up to 1000 model runs. The methods examined range from recent metamodelling approaches to elementary effects and Monte Carlo estimators of the Sobol’ total effect index. The results give a comparison in unprecedented depth, and show that on average and in the setting investigated, Monte Carlo estimators, particularly the VARS estimator, outperform metamodels. Indicatively, metamodels become competitive at around 10–20 runs per model input, but at lower ratios sampling-based approaches are more effective as a screening tool.\n\nSupporting code is found here, and the metafunction is integrated into the sobolsens package in R."
  },
  {
    "objectID": "publications/2021_ASEM/index.html",
    "href": "publications/2021_ASEM/index.html",
    "title": "Exploring the link between Asia and Europe connectivity and sustainable development",
    "section": "",
    "text": "About\nAsia and Europe have made connectivity between people, businesses and institutions a top political priority in the frame of the Asia-Europe Meeting (ASEM) intergovernmental cooperation forum. In the ASEM context, policy leaders agreed that improving connectivity between countries should contribute to achieve the Sustainable Development Goals. Connectivity is a complex concept involving multiple dimensions. Yet in order to provide clear data-driven evidence to support policymaking on Asia-Europe sustainable connectivity, this concept calls for a measurement framework. The mismatch between the politically adopted definition of sustainable connectivity and existing metrics, led to the development of the two indexes described in this paper. Taken together, they help to explore and understand the relationship between connectivity and sustainability. The results suggest that connectivity and sustainability policy agendas have the potential to mutually reinforce one another. Higher values in connectivity are strongly associated with higher values in the social dimension of sustainability. There are evident gaps between European and Asian nations in terms of connectivity and sustainability. However, both continents are underpinned by the common challenge of reducing the environmental impacts of connectivity without neglecting economic/financial sustainability aspects, while at the same time ensuring that benefits will accrue to society at large.\n\nSee the project webpage at the ASEM Sustainable Connectivity Portal. See also AESCON, the follow-up conference to this project."
  },
  {
    "objectID": "publications/2021_ModelSel/index.html",
    "href": "publications/2021_ModelSel/index.html",
    "title": "Variable Selection in Regression Models Using Global Sensitivity Analysis",
    "section": "",
    "text": "About\nGlobal sensitivity analysis is primarily used to investigate the effects of uncertainties in the input variables of physical models on the model output. This work investigates the use of global sensitivity analysis tools in the context of variable selection in regression models. Specifically, a global sensitivity measure is applied to a criterion of model fit, hence defining a ranking of regressors by importance; a testing sequence based on the ‘Pantula-principle’ is then applied to the corresponding nested submodels, obtaining a novel model-selection method. The approach is demonstrated on a growth regression case study, and on a number of simulation experiments, and it is found competitive with existing approaches to variable selection."
  },
  {
    "objectID": "publications/2021_RepresentCIs/index.html",
    "href": "publications/2021_RepresentCIs/index.html",
    "title": "A framework based on statistical analysis and stakeholders’ preferences to inform weighting in composite indicators",
    "section": "",
    "text": "About\nComposite Indicators (CIs, a.k.a. indices) are increasingly used as they can simplify interpretation of results by condensing the information of a plurality of underlying indicators in a single measure. This paper demonstrates that the strength of the correlations between the indicators is directly linked with their capacity to transfer information to the CI. A measure of information transfer from each indicator is proposed along with two weight-optimization methods, which allow the weights to be adjusted to achieve either a targeted or maximized information transfer. The tools presented in this paper are applied to a case study for resilience assessment of energy systems, demonstrating how they can support the tailored development of CIs. These findings enable analysts bridging the statistical properties of the index with the weighting preferences from the stakeholders. They can thus choose a weighting scheme and possibly modify the index while achieving a more consistent (by correlation) index.\n\nSource code is available here."
  },
  {
    "objectID": "publications/2022_Battle/index.html",
    "href": "publications/2022_Battle/index.html",
    "title": "A comprehensive comparison of total-order estimators for global sensitivity analysis",
    "section": "",
    "text": "About\nSensitivity analysis helps identify which model inputs convey the most uncertainty to the model output. One of the most authoritative measures in global sensitivity analysis is the Sobol’ total-order index, which can be computed with several different estimators. Although previous comparisons exist, it is hard to know which estimator performs best since the results are contingent on the benchmark setting defined by the analyst (the sampling method, the distribution of the model inputs, the number of model runs, the test function or model and its dimensionality, the weight of higher order effects, or the performance measure selected). Here we compare several total-order estimators in an eight-dimension hypercube, where these benchmark parameters are treated as random parameters. This arrangement significantly relaxes the dependency of the results on the benchmark design. We observe that the most accurate estimators are from Razavi and Gupta, Jansen, or Janon/Monod for factor prioritization, and from Jansen, Janon/Monod, or Azzini and Rosatifor approaching the “true” total-order indices. The rest lag considerably behind. Our work helps analysts navigate myriad total-order formulae by reducing the uncertainty in the selection of the most appropriate estimator."
  },
  {
    "objectID": "publications/2022_WEFnexus/index.html",
    "href": "publications/2022_WEFnexus/index.html",
    "title": "The Water-Energy-Food Nexus Index: A Tool to Support Integrated Resource Planning, Management and Security",
    "section": "",
    "text": "About\nThe call for measuring synergies and trade-offs between water, energy, and food is increasing worldwide. This article presents the development and application of a country-level index that has been calculated for 181 nations using open databases. Following an assessment of 87 water-, energy-, and food-related indicators, 21 were selected to constitute the Water-Energy-Food (WEF) Nexus Index. In this article, the WEF Nexus Index is utilized to assess the Southern African Development Community, where it demonstrates that food security is an area of concern, while the potential for beneficially exploiting water resources and energy projects exists in several countries. Water for agriculture could be achieved through the drought-proofing of rainfed agriculture and systematic irrigation development, with energy as the critical enabler. Neither the composite indicator nor the WEF nexus approach is the panacea that will solve all the significant development or environmental challenges facing humanity. However, they could contribute to integrated resource management and policy-making and are complementary to the Sustainable Development Goals. In this study, the methodology set out by the Joint Research Centre’s Competence Center on Composite Indicators and Scoreboards has been followed. A set of visualizations associated with the WEF Nexus Index have been compiled in an interactive website, namely www.wefnexusindex.org.”\n\nSee interactive data exploration at the WEF Nexus Index homepage."
  },
  {
    "objectID": "posts/2024-04-10_composer_release/index.html",
    "href": "posts/2024-04-10_composer_release/index.html",
    "title": "Launch of the {composer} composite indicator app",
    "section": "",
    "text": "This article introduces the composer app, which is an easy-to-use web app for building and analysing composite indicators. {composer} has been built as part of a project with FIND, a global non-profit connecting countries and communities, funders, decisionmakers, healthcare providers and developers."
  },
  {
    "objectID": "posts/2024-04-10_composer_release/index.html#an-app-for-what",
    "href": "posts/2024-04-10_composer_release/index.html#an-app-for-what",
    "title": "Launch of the {composer} composite indicator app",
    "section": "An app for what?",
    "text": "An app for what?\nComposite indicators are all around us, but unless you’ve been deeply embroiled in international statistics, you may never have heard of them. So let’s start at the beginning and talk about indicators first.\nIndicators are used in many contexts, and you may have heard of “key performance indicators” (KPIs) and sustainable development indicators, among other examples. Without agonising over definitions too much, indicators are things we can measure, that indicate the state of something we are interested in. For example a, KPI can indicate the performance or progress of a project or spending programme. Sustainable development indicators indicate the progress of countries towards the UN’s sustainable development goals. And so on.\nNotice that indicators are different from direct measurements. If we want to know how hot it is outside, we can measure the temperature directly with a thermometer. Indicators are used instead where we cannot directly measure the concept we are interested in, usually because:\n\nIt is multidimensional\nIt is not precisely defined\n\nIn general we can say that indicators are used to measure complex (often socioeconomic) multidimensional concepts, with the aim of prioritising resources and interventions, and also to track progress and raise awareness. In the international/policy context, indicators are often used to compare countries and/or sub-national regions, but indicators can be used to compare/measure universities, businesses, or even individuals. We’ll refer to the things we are comparing with our indicators as units in many places in this article.\n\n\n\nEuropean social scoreboard\n\n\nIn most cases, indicators come in groups, attempting to measure different aspects of performance, development, etc. Groups of indicators are called “scoreboards”, or “dashboards”, depending on your favourite terminology. However, when we get beyond a small number of indicators, it can be increasingly difficult to make sense of the information, especially when the dashboard regards multiple countries/units."
  },
  {
    "objectID": "posts/2024-04-10_composer_release/index.html#composite-indicators",
    "href": "posts/2024-04-10_composer_release/index.html#composite-indicators",
    "title": "Launch of the {composer} composite indicator app",
    "section": "Composite indicators",
    "text": "Composite indicators\nEnter composite indicators. Composite indicators are mathematical aggregations of a set of indicators into a single measure. They effectively act as summary measures of the indicators within a dashboard or scoreboard, and facilitate quick and easy comparisons, clear communication with stakeholders, and act as a natural entry point to the data set underneath. Well-known examples of composite indicators include the Human Development Index, the Global Innovation Index, and many others. Composite indicators are often built based on conceptual framework, which maps indicators into dimensions and sub-dimensions.\n\n\n\nAn example of an indicator scoreboard: the European social scoreboard\n\n\nIn the context of FIND, indicators are used to better understand where there is the most need to direct resources, for example which countries are most in need of funding, and for which type of medical diagnostics. In this context, composite indicators can also provide an important overview of a potentially complex set of indicators.\nImportantly, in building a composite indicator, we do not wish to substitute the underlying data, but rather to complement it with an overview measure. Composite indicators involve a number of subjective decisions in their construction, and cannot fully capture all information in the indicator set underneath. However, used carefully, they are a valuable addition and entry point to a complex data set."
  },
  {
    "objectID": "posts/2024-04-10_composer_release/index.html#steps-to-building-a-composite-indicator",
    "href": "posts/2024-04-10_composer_release/index.html#steps-to-building-a-composite-indicator",
    "title": "Launch of the {composer} composite indicator app",
    "section": "Steps to building a composite indicator",
    "text": "Steps to building a composite indicator\nBuilding a composite indicator is not a mathematically complicated task. However, it does need care in terms of data management, and in making the methodological choices that fit with the objectives of the index. The European Commission’s Competence Centre on Composite Indicators and Scoreboards gives a ten-step approach to building composite indicators which is as follows:\n\nDefine the concept to be measured - make a map of your concept.\nSelect the indicators, keeping in mind criteria such as data availabililty, relevance, reliability.\nAnalyse the data, checking the distributions and key statistics of each indicator. Treat outliers if necessary.\nNormalise indicators onto a common scale, reversing directions if needed.\nAssign weights to indicators and dimensions.\nAggregate the normalised indicators up to the index scores.\nCheck statistical coherence using correlation analysis.\nAssess the impact of uncertainties by running a sensitivity analysis.\nMake sense of the results to reveal narratives and conclusions from the data.\nPresent and communicate the results to stakeholders in a clear and focused way, using graphics and key messages.\n\nKeep in mind these steps are guidelines, and may not fit every context. But they are certainly a good place to start.\nAlthough some of the steps in this process are qualitative, such as mapping the concept, steps 2-10 all involve at least an element of data analysis, processing and visualisation.\nThis is where the Composer app comes in. {Composer} is an interactive web app which walks you through the data steps of building a composite indicator, enabling fast prototyping, results and visualisation. Read on to find out more!"
  },
  {
    "objectID": "posts/2024-04-10_composer_release/index.html#composer-a-new-web-app",
    "href": "posts/2024-04-10_composer_release/index.html#composer-a-new-web-app",
    "title": "Launch of the {composer} composite indicator app",
    "section": "{composer}: a new web-app",
    "text": "{composer}: a new web-app\n{composer} is an web-app built in Shiny, and is hosted by FIND here. It can do the following things for you:\n\nAny number of indicators and units and aggregation levels\nUnit screening by data availability\nMissing data imputation\nOutlier treatment\nNormalisation using various methods\nWeighted aggregation\nInteractive maps (if the units are countries)\nDetailed analysis of indicators using bubble charts, bar charts\nStatistical analysis using visualisation of distributions, correlation plots\nDownloadable unit profiles\nInteractive reweighting\nSensitivity analysis on assumptions, checking the effects of removing indicators and indicator groups\n\nWhat composer doesn’t do:\n\nIt doesn’t help you map your concept, as this is a qualitative task. However, it does provide a visualisation of your concept map.\nIt won’t source data for you. You have to provide it with your indicator data.\nIn general, it provides analysis from a statistical point of view. It cannot tell you anything about, for example, the relevance of your indicators. Because again, this cannot be inferred from the data.\n\nMost of the data processing in the app is done using the COINr package, which is an R package for building and analysing composite indicators. However, {composer} is a convenient graphical interface which removes the need to do any programming, so it is very easy to get started! If you are an R user though, you may wish to additionally work with COINr directly to access functionalities not included in the app. And/or, since {composer} is an R package, you can download and install it, and run the app locally from R with the following command:\n\n# install package\nremotes::install_github(\"finddx/composer\")\n\n# load package\nlibrary(composer)\n\n# run app\nrun_gui()"
  },
  {
    "objectID": "posts/2024-04-10_composer_release/index.html#before-you-start",
    "href": "posts/2024-04-10_composer_release/index.html#before-you-start",
    "title": "Launch of the {composer} composite indicator app",
    "section": "Before you start",
    "text": "Before you start\nWe’re nearly at the point of using the app, but not quite, because you need to have a few things ready before you start! The starting point is to provide the app with an Excel spreadsheet containing two tables:\n\nThe indicator data, containing indicator values for each unit.\nThe indicator metadata, which specifies the structure of the index, initial weights, indicator names and so on.\n\nThese tables need to follow a specific format, so they require a little care to get right. However, they are very clearly explained in the online documentation, and you can also take a look at the example dataset.\nWhen you have this spreadsheet ready you can upload it to the app and get started."
  },
  {
    "objectID": "posts/2024-04-10_composer_release/index.html#medical-diagnostics-example",
    "href": "posts/2024-04-10_composer_release/index.html#medical-diagnostics-example",
    "title": "Launch of the {composer} composite indicator app",
    "section": "Medical Diagnostics Example",
    "text": "Medical Diagnostics Example\nThe best way to showcase the app is with an example. We’ll start with the Pathogen Diagnostics Readiness Index (PDxRI), which is a simple composite indicator of seven indicators, and measures the global readiness to deal with a set of diseases. Later we will also show an example which benchmarks countries in terms of resources.\n\nThe Pathogen Diagnostics Readiness Index (PDxRI) is a comprehensive tool and index for evaluating diagnostic preparedness by assessing the availability of diagnostics globally. The tool aims to highlight the global availability of diagnostic tools, raise awareness about existing gaps in the diagnostic technology landscape, and guide investments for the future development of diagnostic tools.\n\n\n\n\nConceptual framework of the PDxRI\n\n\nIn this example, the “units” are the pathogens - therefore the purpose of the index is to rank pathogens in terms of diagnostic preparedness. In {composer}, our objective will be to reproduce the results of the index quickly in the app.\nWe begin by loading the data set which is available here using the Data Upload interface on the first page of the app. The example data set is already in the correct format for {composer}: an Excel spreadsheet with two tabs, containing in the indicator data and metadata tables respectively. On loading this data, the app recognises the structure of the index an returns a framework plot:\n\n\n\nConceptual framework of the PDxRI (app visualisation)\n\n\nIn order to reproduce the index, we follow the methodological steps as described on the index website:\n\nTreat outliers using Winsorisation.\nNormalise onto a 0-100 scale.\nUse the arithmetic mean to calculate the index scores.\n\nThese options are all available directly in the app. We begin therefore by treating the outliers. To do this, we go to the “Data operations” dropdown menu, and select the “Outlier treatment” tab. This tab simply gives us the option to treat or not treat outliers, using a built-in algorithm based on Winsorisation and a log transform. The results of this process are displayed via the table and summary boxes.\n\n\n\nOutlier treatment results\n\n\nThe clear outlier in this data set is COVID, which has received far more attention than any of the other pathogens, and therefore has much higher scores. As such, it is treated as an outlier.\nThe app works sequentially, so at this point we have now treated our outliers and can move the next operation, which is normalisation. We therefore go to the “Normalise” tab using the navigation menu at the top. To replicate the methodology we select “Min-max” normalisation, and set the min and max to 0 and 100 respectively.\n\n\n\nNormalised data set\n\n\nThe app has now normalised each indicator using the specified min-max rule, and displays the modified data in the table. Note that the normalisation step also accounts for the directions of indicators.\nOur final step is simply to aggregate the normalised scores using the “Compose index” tab. Here, we select the weighted arithmetic mean, the “original” weights (which means those specified in the input spreadsheet), and set the minimum data availability to zero since we don’t want to apply any data availability rules here.\n\n\n\nAggregated index scores\n\n\nRunning this produces the results table above, which is colour coded by the index scores, and exactly reproduces the results of the index (note: updates to this data set may eventually change the results).\nAt this point, the results of the index can already be exported to Excel by clicking on the “Save/export” dropdown in the top right. You can also export the results to R and the R commands to build the index (see later in this article).\nHere we will just demonstrate a couple of further features of the app. The “Indicator statistics” tab gives us a variety of indicator statistics and plots which allow us to dig deeper into the data and the relationships between indicators:\n\n\n\nIndicator statistics and plots\n\n\nTo make detailed plots of one indicator (or the index) against another, the “Bubble chart” interface provides a powerful tool for plotting bubble charts, and additionally colouring by grouping variables such as transmission mode.\n\n\n\nBubble chart example\n\n\nThis example has shown how a composite indicator can be quickly created and analysed in {composer}. We have only explored a few of the features of the app here, but the online documentation provides a full description. In the next example we will explore some other features of the app including the possibility to build maps when working with country data."
  },
  {
    "objectID": "posts/2024-04-10_composer_release/index.html#wef-nexus-example",
    "href": "posts/2024-04-10_composer_release/index.html#wef-nexus-example",
    "title": "Launch of the {composer} composite indicator app",
    "section": "WEF Nexus Example",
    "text": "WEF Nexus Example\nThe WEF Nexus Index is a national-level composite indicator which aims to measure the state of countries in terms of their resources in Water, Energy and Food (WEF). It consists of 21 indicators, organised into three dimensions and six sub-dimensions, as shown below.\n\n\n\nConceptual framework of the WEF Nexus Index\n\n\nThe data input for {composer}, for this index, is available to download here. Uploading this spreadsheet to the app, we get the visualisation for the framework:\n\n\n\nConceptual framework of the WEF Nexus Index (composer visualisation)\n\n\nIf you follow this example, you will also notice that the app recognises that we are dealing with country-level data because in this example, the unit codes are ISO alpha-3 country codes (AFG, AGO, ALB, …). This means we can visualise the results on a map later.\nIf we want to recreate the WEF Nexus Index from this data, we must follow these steps:\n\nTreat outliers\nNormalise using min-max, scaled between 0 and 100.\nAggregate using the arithmetic mean (but with data availability threshold).\n\nTo do this, we go to the Outlier Treatment tab and click “Run”. This shows that seven indicators require data treatment, and some are log-transformed since they are very skewed. Recall that outlier treatment is applied to avoid having indicators that are dominated by some few outlying points. You can read more about this in the relavant documentation page.\n\n\n\nTreating outliers in the WEF Nexus Index\n\n\nNext we go to the Normalisation tab. We select the “min-max” method, with lower bound 0 and upper bound 100, following WEF Nexus Index methodology. This now transforms all indicators onto the \\([0,100]\\) interval, and reverses directions where necessary. For example, the indicator “Annual freshwater withdrawals” is a negative-direction indicator (higher values are worse), and so it is flipped at this point. After normalisation, all indicators are aligned such that higher values are better.\nAs the final calculation step, we aggregate the normalised scores to get the sub-dimension, dimension and index scores. A recent change to WEF Nexus Index methodology specifies that for each country, it must have a minimum of 2/3 of its indicator values available with any given sub-dimension to calculate a sub-dimension score. We can replicate this by setting the “Min. data availability” bar to 0.66. Now we click “Run” to get the results.\n\n\n\nWEF Nexus Index results\n\n\nThe results table shows at a glance the top countries, and the colour formatting helps to understand also why a given country is where it is. For example, here we see that Iceland is top because it has a very high energy score, both in terms of availability and access.\nTo explore the results in more depth we can begin by plotting the index results on a “choropleth” map.\n\n\n\nWEF Nexus Index results a map (green-blue implies higher scores, red is lower.)\n\n\nThis helps to understand the geographic trends: we see that central African countries, and many in the Middle-East, seem to have the lowest scores. Remember that the mapping feature only works in the app if the unit codes are ISO alpha-3 codes.\n\n\n\nCountry profile for Ethiopia\n\n\nGoing to the “Unit profiles” tab, we can see the scores by country. The figure above shows Ethiopia: where it lies in the global ranking, and its scores and ranks for the main index and the (sub-)dimensions.\n\n\n\nCountry profile for Ethiopia\n\n\nThe top and bottom-ranking indicator tables also help to explain the position of each country in the rankings. Here we see that Ethiopia actually scores quite well in terms of energy indicators, and also in terms of having low CO2 emissions. However on water and food, it scores poorly in comparison to other countries.\nThe country profiles are important because they take us back to the underlying data, which consists of real measurements, and explains why a country may score high or low in the index."
  },
  {
    "objectID": "posts/2024-04-10_composer_release/index.html#what-else",
    "href": "posts/2024-04-10_composer_release/index.html#what-else",
    "title": "Launch of the {composer} composite indicator app",
    "section": "What else?",
    "text": "What else?\nThis blog post has just covered some of the main features of the app. But {composer} actually does more! For example, in the “Save/export” menu at the top of the page, you have some very useful options:\n\nYou can export your results, and the data as it is processed at each step, to Excel.\nYou can bookmark your progress. Clicking on this link will generate a URL, which will take you back to where you left off last time!\nYou can export to R, either by saving the “coin” to an RDS file, or the app will even generate a COINr script which reproduces your results in R using the COINr package!\n\nThis last point is especially useful if you need more advanced features that aren’t available in the app.\nIf you want to know more about {composer}, read the online documentation, and check out the source code at its GitHub repo."
  }
]